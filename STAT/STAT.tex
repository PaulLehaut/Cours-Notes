\documentclass{article}

\title{Statistiques et Analyse de Données}
\author{Paul Lehaut}
\usepackage{mesraccourcis}

\begin{document}
\maketitle
\newpage
\tableofcontents
\newpage
\section{Rappels}
\subsection{Espérance et (Co)Variance}
On considère dans cette section des VA$\R$.
\bigskip

\Def 
\sskip 
L'espérance de $X\sim\Prob$ est l'intégral de Lebesgue:
$$\E (X):=\int_{\omega\in\Omega}X(\omega)d\Prob (\omega)=\int_{x\in\R}xd\Prob(x)$$
qui est bien définie si X est intégrable, c'est-à-dire si $\E(|X|)<+\infty$.
\sskip 
Si par ailleurs $\E(X^2)<+\infty$, alors la variance de $X$ est bien définie par:
$$\V (X):=\E((X-\E(X))^2)=\E(X^2)-\E(X)^2.$$
\bskip
L'inégalité de Jensen donne alors, pour X une VA$\R^d$ intégrable, soit $f$ une fonction à valeurs réelles convexe définie sur $\R^d$, alors $\E(f(X))$ est bien définie et:
$$f(\E(X))\le \E(f(X)).$$
\bigskip

\Def 
\sskip 
La covariance entre $X$ et $Y$, telles que $\E(X)<+\infty$ et $\E(Y)<+\infty$, est définie par:
$$Cov(X,Y):=\E((X-\E(X))(Y-\E(Y)))=\E(XY)-\E(X)\E(Y)$$
on a alors l'identité remarquable suivante:
$$\V (X+Y)=\V(X)+2Cov(X,Y)+\V(Y).$$
\bskip 
Lorsque $X$ est une VA$\R^d$ de carré intégrable, on peut définir sa matrice de covariance qui est symétrique positive.
\bigskip

\Def 
Le coefficient de corrélation de $X$ et $Y$ est définie par:
$$\rho_{X,Y} :=
\left\{
\begin{array}{ll}
\dfrac{\operatorname{Cov}(X,Y)}{\sqrt{\operatorname{Var}(X)} \, \sqrt{\operatorname{Var}(Y)}} \in [-1,1] & \text{si } \operatorname{Var}(X)\operatorname{Var}(Y) > 0, \\[1.5ex]
0 & \text{sinon.}
\end{array}
\right.$$
\bigskip
\subsection{Indépendance}

\Def 
Une famille finie de variables aléatoires $(X_1,...,X_n)$ définies sur $(\Omega, \mathcal{F})$ est indépendante si pour toute famille de sous-ensembles mesurables $(C_1,...,C_n)$ on a:
$$\Prob(X_1\in C_1,...,X_n\in C_n)=\Prob(X_1\in C_1)...\Prob(X_n\in C_n)$$
ou, de façon équivalente, si:
$$\Prob =\Prob_{X_1}\otimes...\otimes\Prob_{X_n}.$$
\bskip 
Si $\indn{X}$ sont indépendantes alors, pour toutes fonctions mesurables $\indn{f}$ telles que $f_1(X_1),...,f_n(X_n)$ soient intégrables, on a:
$$\E(f_1(X_1),...,f_n(X_n))=\E(f_1(X_1))...\E(f_n(X_n)).$$
\bigskip

\subsection{Variables Aléatoires Discrètes}
Si E est discret, toutes mesures de probabilités est caractérisée par la famille de nombres ($p(x), \ x\in E$). Les variables aléatoires principales sont définies dans le polycopié.
\bigskip

\subsection{Variables Aléatoires à Densité}
Les intégrales sont ici à considérer au sens de Lebesgue.
\bigskip

\Def 
\sskip
Une variable aléatoire $X$ est dite à densité p si:
$$\forall C \in \mc{B}(\R^d), \Prob(X\in C)=\int_{x\in C}p(x)dx$$
une fonction mesurable et positive p est une densité de probabilité si et seulement si:
$$\int_{x\in \R^d}p(x)dx=1.$$
\bskip
Des variables aléatoires $\indn{X}$ sont indépendantes si le vecteur aléatoire $(\indn{X})$ a pour densité
\newline
$p=p_{X_1}\otimes...\otimes p_{X_n}$.
\bigskip

\Theo \ Formule de transfert
\sskip 
Si X a une densité $p$, alors pour toutes fonctions mesurables $f$ telle que $\E(|f(X)|)<\pinf$, alors:
$$\E(f(X))=\int_{x\in \R^d}f(x)p(x)dx.$$
\bskip
Les exemples principaux de variables aléatoires à densité se trouvent dans le polycopié.
\bigskip

\subsection{Somme de VAs indépendantes}
Soient $X$ et $Y$ deux VAs indépendantes, on note $Z:=X+Y$.
\bigskip

\Prop 
\sskip
La densité de Z est:
$$r(z)=\int_{x\in\R^d}p_X(x)p_{Y}(z-x)dx$$
on l'appelle la convolution de $p_X$ et $p_Y$.
\bigskip

\subsection{Fonction de Répartition}
On se place dans le cas où E=$\R$.
\bigskip

\Def 
\sskip
La fonction de répartition d'un variable aléatoire $X$ est définie par:
$$\forall x\in \R, \ F(x)=\Prob(X\le x)$$
alors, pour tout réel $r\in (0,1)$, un quantile d'ordre r pour $X$ est un nombre $q_r$ tel que:
$$\Prob(X\le q_r)=F(q_r)=r.$$
\bskip 
En général un quantile n'existe pas toujours ou n'est pas unique, néanmoins, lorsque $X$ possède une densité qui est positive alors le quantile existe et est unique.
\bigskip

\Prop 
\sskip
Si $X$ est une VA$\R$ à densité $p$, alors sa fonction de répartition est continue et dérivable \pp, sa dérivée est \pp \ égale à sa fonction de densité.
\bigskip

\subsection{Fonction Charactéristique}
Soient $X$ et $Y$ des vecteurs aléatoires.
\bigskip

\Def \ Fonction Charactéristique
\sskip 
La fonction charactéristique de X est la fonction $\Psi_X : \R^d\longrightarrow \C$ définie par:
$$\Psi_X(u):=\E(e^{i<u,X>})=\E(\cos(<u,X>))+i\E(\sin(<u,X>)).$$
\bigskip 

\Prop 
\sskip
Si, pour tout $u\in\R^d$, $\Phi_X(u)=\Phi_Y(u)$, alors $X$ et $Y$ sont de même loi.
\bigskip

\Prop 
\sskip
Si $X\sim \mc{N}(\mu,\sigma^2)$ et $Y\sim \mc{N}(\nu,\tau ^2)$ sont indépendantes, alors:
$$X+Y\sim \mc{N}(\mu+\nu,\sigma^2+\tau^2).$$
\bigskip

\subsection{Vecteurs Gaussiens}
Soit $X$ un vecteur aléatoire.
\bigskip

\Def \ Vecteur Gaussien
\sskip 
$X$ est un vecteur Gaussien si, pour tout vecteur $u$, la variable aléatoire $<u,X>$ est gaussienne.
\bskip 
De ce qui précède, on déduit que, en notant $\E(X)=m$ et $Cov(X)=K$, alors:
$$<u,X>\sim\mc{N}(<u,m>,<u,Ku>) \ \ \text{et } \ \Phi_X(u)=\exp(i<u,m>-\frac{1}{2}<u,Ku>).$$
Par ailleurs, si $K$ est inversible, alors $X$ a la densité:
$$\frac{1}{\sqrt{(2\pi)^d\det K}}\exp(-\frac{1}{2}<x-m,K^{-1}(x-m)>)$$
sinon $X$ n'a pas de densité.
\bigskip

\subsection{Théorèmes de Convergence}
Soient $(X_n)$ et $X$ des vecteurs aléatoires.
\bigskip

\Def
\sskip
$(X_n)$ converge vers $X$ presque sûrement si: $\Prob(\lim X_n =X)=1$.
\sskip 
$(X_n)$ converge vers $X$ en probabilité si, pour tout $\epsilon>0, \ \lim\Prob(||X_n-X||\ge \epsilon)=0$.
\sskip 
$(X_n)$ converge vers $X$ en distribution si, pour toute fonction continue et bornée $f:\R^d\longrightarrow \R $, $\E(f(X_n))$ converge vers $\E(f(X))$.
\bigskip 

\Theo \ Convergence Dominée
\sskip 
Supposons que $(X_n)$ converge vers $X$ presque sûrement et qu'il existe $Y$ positive et intégrable telle que: $||X_n||\le Y$ presque sûrement, alors $\E(X_n)$ converge vers $\E(X)$.
\bskip 
Pour $f$ une fonction continue, si $(X_n)$ converge presque sûrement (respectivement en probabilité ou en distribution) vers $X$ alors $f(X_n)$ converge presque sûrement vers $f(X)$ (respectivement en probabilité ou en distribution).
\bigskip

\Prop 
\sskip 
$(X_n)$ converge en distribution (c'est-à-dire en loi) vers $X$ si et seulement si $\Psi_{X_n}(u)$ converge vers $\Psi_X(u)$ pour tout u.
\sskip
La convergence presque sûre implique la converge en probabilité qui implique elle même la converge en distribution.
\bskip
Si on se place dans le cadre réel, alors on a les équivalences suivantes:

-$(X_n)$ converge vers $X$ en distribution

-$\Prob (X_n\le x)\longrightarrow \Prob(X\le x)$ pour tout $x$ tel que $\Prob(X=x)=0$

-$\Prob (X_n< x)\longrightarrow \Prob(X< x)$ pour tout $x$ tel que $\Prob(X=x)=0$ 
\bskip 
On dit que la suite $(X_n)$ est indépendante et indentiquement distribuée si ses variables sont indépendantes et de même loi. On note alors:
$$\bar{X_n}=\frac{1}{n}\sum_{i=1}^nX_i$$
la moyenne empirique de $\indn X$.
\bigskip 

\Theo \ La Loi Forte des Grands Nombres
\sskip
Soit $(X_n)$ une suite de VA$\R^d$ iid telle que $\E(||X_1||)<\pinf$, alors:
$$\lim \bar{X_n}=\E(X_1) \ \ \text{presque sûrement}.$$
\bigskip

\Theo \ Théorème Central Limite Multivarié
\sskip 
Soit $(X_n)$ une suite de VA$\R^d$ iid telle que $\E(||X_1||^2)<\pinf$, alors:
$$\lim\sqrt{n}(\bar{X_n}-\E(X_1))=\mc{N}(0,Cov(X_1)) \ \ \text{en distribution.}$$
\newpage
\section{Estimateur dans un Modèle Paramétrique}
On considère un échantillon $\indn{X}$ de VA iid dans un espace mesurable $(E,\mc E)$ de loi $\Prob$ inconnue qu'on cherche à éclaircir. On cherche par exemple à estimer $\E_\Prob(X_1), \ V_\Prob(X_1)$ l'histogramme de $\Prob$ ou encore d'autre quantité d'intérêt (QI).
\bskip
On se restreint à des lois d'une certaine forme caractérisées par un ou certain paramètres. Formellement, on considère une famille de lois: $\{\Prob_\theta; \ \theta \in \Theta\}$.
\bigskip 

\subsection{Estimateurs}
On cherche donc à estimer une QI à l'aide de notre échantillon de VA, pour ce faire on va chercher à approcher QI par une fonction de l'échantillon appelée statistique.
\bigskip 

\Def 
\sskip 
Une statistique $T_n$ est une VA de la forme $T_n=t_n(\indn X)$ avec $t_n$ déterministe et qui ne dépend pas de $\Prob$.
\sskip
On appelle un estimateur une statistique qui vise à approcher une certaine QI.
\bigskip 

\Def 
\sskip 
Un estimateur $Z_n$ d'une QI est dit constistant si $Z_n$ converge en probabilité vers QI, il est fortement consistant s'il converge vers QI \ps.
\bskip 
Par exemple, dans le cas où $E=\R$, alors la moyenne empirique $\bar X_n=\frac 1 n\sum_{i=1}^nX_i$ est un estimateur fortement consistant de $\E(X_1)$.
\bigskip

\subsection{Biais et Risque Quadratique}
Le biais et la MSE permettent de quantifier la distance d'un estimateur à sa QI dans un régime non asymptotique (ie n est fini).
\bigskip 

\Def 
\sskip 
Soit un estimateur intégrable $Z_n$, le biais de $Z_n$ est:
$$b(Z_n)=\E(Z_n)-QI, \ \ \text{il s'agit de la distance moyenne de l'estimateur à la QI}$$
si ce biais est nul, on dit que l'estimateur est non biaisé.
\bskip 
Il est intéressant de remarquer que la variance empirique est biaisée ce qui motive la définition suivante:
\bigskip 

\Def 
\sskip 
Si $E=\R$, l'estimateur non-biaisé de la variance est:
$$S_n^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar X_i)^2.$$
\bskip 
Une mesure plus précise de la distance de l'estimateur à la QI peut être donnée par la MSE.
\bigskip 

\Def 
\sskip 
Soit $Z_n$ un estimateur de carré intégrable, la MSE de $Z_n$ est définie par:
$$MSE(Z_n)=E(||Z_n-QI||^2).$$
\bigskip 

\Prop 
\sskip 
On peut a l'égalité suivante:
$$MSE(Z_n)=||b(Z_n)||^2+V(Z_n).$$
\bskip 
En général on ne peut pas minimiser à la fois la variance et le biais. En data science il peut être intéressant d'introduire un biais pour réduire la variance du modèle et donc le risque d'overfitting.
\bigskip 

\subsection{Modèle Paramétrique et Estimation du Moment}
On s'intéresse ici à l'estimation de la distribution complète $\Prob$ de $X_1$. Il y a deux approches principales: la méthode non paramétrique (par histogramme) et la méthode paramétrique qui repose sur la supposition que $\Prob$ a une certaine forme (comme exponentielle ou gaussienne).
\bigskip 

\Def 
\sskip 
Un modèle paramétrique sur E est un ensemble de mesures de probabilités:
$$\mc P=\{\Prob_\theta; \ \theta\in\Theta\}$$
sur l'espace E, indexé par un ensemble de paramètres $\Theta\subset\R^k$.
\bskip 
Il est important de constater que si, pour deux valeurs distinctes $\theta$ et $\theta'$, on a $\Prob_\theta = \Prob_{\theta'}$ alors il n'est pas possible de distinguer $\theta$ de $\theta'$ par simple observation de $\indn X$. On travaillera donc toujours en supposant que la fonction $\theta\mapsto\Prob_\theta$ est injective, on dira alors que $\mc P$ est identifiable.
\sskip 
On fixe désormais un modèle paramétrique $\mc P$. Pour tout $\theta$ il est pratique de dénoté par $\Prob_\theta$ la mesure de probabilité pour laquelle, pour tout $n\ge 1$, les VAs $\indn X$ sont iid selon $\Prob_\theta$. On définit de façon similaire $\E_\theta, \ V_\theta$ etc.
\bigskip 

\Prop 
\sskip
Soit X une VA$\R$ non déterministe intégrable et qui prend ses valeurs dans un intervalle I. Soit $\Phi : I\to \R$ strictement convexe, alors: $\Phi(\E(X))<\E(\Phi(X))$.
\bskip 
La méthode des moments est une procédure naturelle pour construire des estimateurs, on détaille ici l'exemple pour le modèle exponentielle: $\{\epsilon(\lambda); \ \lambda >0\}$ dans lequel on cherche un estimateur de $\lambda$:
\begin{itemize}
\item La loi forte des grands nombres donne, pour tout $\lambda>0$: $\bar X_n\to\E_\lambda(X_1)=\frac 1 \lambda$ \ps, donc $\frac{1}{\bar X_n}\to \lambda$ \ps.
\newline
La continuité de la fonction $x\mapsto \frac 1 x$ sur $\R_+^*$ assure donc que $\bar\lambda_n =\frac{1}{\bar X_n}$ est un estimateur fortement consistant de $\lambda$.
\end{itemize}
La généralisation abstraite de cette méthode s'énonce de la façon suivante:
\begin{itemize}
\item Pour l'estimation de $g(\theta)\in  \R^d$, cela consiste à trouver $\phi$ et $m$ des fonctions telles que:
$$\forall \theta\in \Theta, \ \E_\theta(\phi(X_1)) = m(g(\theta)).$$
Pour le modèle exponentielle on a pris: $\phi(x) =x, \ g(\lambda)=\lambda$ et $m(\lambda)=\frac{1}{\lambda}$.
\newline 
Alors, la loi forte des grands nombres nous permet donc d'approximer $m(g(\theta))$ par: $\frac{1}{n}\sum_{i=1}^n\phi(X_i)$ de sorte que, si $m$ possède une fonction réciproque continue $m^{-1}$, alors:
$$Z_n=m^{-1} (\frac{1}{n}\sum_{i=1}^n\phi(X_i))$$
est un estimateur fortement consistant de $g(\theta)$.
\end{itemize}
\bigskip 

\subsection{Convergence Normale}
La construction d'un estimateur par la méthode des moments dépend du choix arbitraire de la fonction $\phi$, donc différents choix de $\phi$ peuvent donner différents estimateurs qui sont tous, par construction, fortement consistant. Pour déterminer l'estimateur le plus intéressant en pratique, on peut s'intéresser à celui qui converge 'le plus vite' vers la QI. Cette vitesse de convergence peut être mesurée à l'aide de la notion de variance asymptotique.
\bigskip 

\Def 
\sskip 
Un estimateur consistant $Z_n$ de $g(\theta)$ est asymptotiquement normal si, pour tout $\theta$, il existe une matrice symétrique positive $K(\theta)\in \M_d(\R)$ telle que: $\sqrt{n}(Z_n-g(\theta))$ converge en distribution vers $\mc N_d(0,K(\theta))$.
\sskip 
La fonction $\theta\mapsto K(\theta)$ est appelé la covariance asymptotique de $Z_n$.
\bigskip 

\Theo \ Méthode delta
\sskip 
Soit $(\zeta_n)$ une suite de VA à valeurs dans $\R^k$ et $a\in \R^k$ telles que $\zeta_n\to a$ en probabilité et $\sqrt{n}(\zeta_n-a)$ converge en distribution vers un vecteur aléatoire $Y\in\R^k$. Soit $\mc U$ un ouvert de $\R^k$ qui contient $a$, soit $\Phi :\mc U\to \R^d$ de classe $\mc C^1$, alors:
$$\lim_{n\to\pinf}\sqrt{n}(\Phi(\zeta_n)-\Phi(a))=\nabla\Phi(a)Y$$
en distribution.
\bigskip 

\Theo \ Slutsky
\sskip 
Soit $((X_n,Y_n))$ une suite de couples de VAs telle que $(X_n)$ converge en probabilité vers une variable déterministe $a$ et que $(Y_n)$ converge en distribution vers une variable aléatoire $Y$. Alors $((X_n,Y_n))$ converge en distribution vers $(a,Y)$, et, par conséquent, pour toute fonction continue $\Psi$, $(\Psi(X_n,Y_n))$ converge en distribution vers $\Psi(a,Y)$.
\bigskip 

\section{Statistiques dans des Modèles Gaussiens}
On commence par quelques rappels.
\bigskip 

\Def 
\sskip 
Un vecteur aléatoire $G\in\R^n$ est un vecteur gaussien standard si chacune de ces composantes est de loi $\mc N(0,1)$ et si elles sont indépendantes.
\sskip 
La fonction caractéristique d'un vecteur gaussien standard est: $\psi_G(u)=e^{-\frac{||u||^2}{2}}$ (il s'agit du cas particulier de la fonction caractéristique d'un vecteur gaussien: $\phi_X(x)=\exp(it^Tm-\frac 1 2 t^T\Sigma t)$).
\bigskip 

\Theo \ Cochran 
\sskip 
Soit $G\sim \mc N_n(0,I_n)$, pour tout sous-\ev \ E de $\R^n$, les coordonnées de G dans toutes base orthonormée de E forment un vecteur gaussien standard.
\bigskip 

\Def 
\sskip 
Pour $n\ge 1$, la distribution $\chi$-carré avec n degrés de liberté, notée $\chi_2(n)$, est la loi de la VA:
$$Z_n=\sum_{i=1}^nG_i^2=||G|| \ \ \text{avec}\ \ G\sim \mc N_n(0,I_n).$$
Son espérance est $n$, sa variance $2n$.
\bigskip 
\subsection{Statistiques d'Echantillons Gaussiens}
On note $S_n^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar X_i)^2$ l'estimateur non biaisé de la variance.
\bigskip 

\Prop 
\sskip 
En considérant $\Prob_{\mu,\sigma^2}$, alors les estimateurs $\bar X_n$ et $S_n^2$ sont indépendants et:
$$\bar X_n \sim \mc N(\mu,\frac{\sigma^2}{n}) \ \ \text{et} \ \ (n-1)\frac{S_n^2}{\sigma^2}\sim \chi_2(n-1).$$
\bskip 
On introduit pour la suite la variable aléatoire réduite $X_i'=\frac{X_i-\mu}{\sigma}\sim\mc N(0,1)$ et on définit comme on s'y attend $\bar{X_n'}$ et $S_n'^2$ telles que:
$$\bar X_n=\mu+\sigma \bar{X_n'} \ \ \text{et} \ \ S_n^2=\sigma^2S_n'^2$$
alors, en notant $E_1=\text{Vect}(\begin{pmatrix}
    1\\
    \vdots\\
    1
\end{pmatrix})$, $E_2=E_1^\bot$ et $e=\frac{1}{\sqrt n}\begin{pmatrix}
    1\\
    \vdots\\
    1
\end{pmatrix}$, il vient: 
$$G_{E_1}=<G,e>e=\bar{X_n'}\begin{pmatrix}
    1\\
    \vdots\\
    1
\end{pmatrix} \ \ \text{et} \ \ ||G_{E_2}||^2=(n-1)(S_n')^2.$$
\bigskip 

\subsection{La Distribution de Student}
Soit $n\ge 1$, alors:
\smallskip 

\Def 
\sskip 
La distribution de Student avec n degrés de liberté, notée $t(n)$, est la loi de la VA: $T_n=Y\sqrt{\frac{n}{Z_n}}$ avec $Z_n\sim \chi_2(n)$ indépendante de $Y\sim \mc N(0,1)$.
\bigskip 

\Prop 
\sskip 
Pour $\Prob_{\mu,\sigma^2}$, alors: $\frac{\bar X_n-\mu}{\sqrt{\frac{S_n^2}{n}}}\sim t(n-1)$.
\bigskip 

\subsection{Régression Linéaire avec Erreurs Gaussiennes}
On s'intéresse ici à $(x_1,y_1),...,(x_n,y_n)\in \R^p\times \R$ et on suppose qu'il existe $\beta\in\R^{p+1}$ et des VAs $\ind \epsilon n$ telles que:
$$y_i = \beta_0 +\beta_1x_i+...+\beta_px_i^p+\epsilon_i.$$
On peut alors réécrire: $Y_n=X_n\beta + \epsilon_n$. L'estimateur de carré minimal (OLS) de $\beta$ est donné par: $\min_\beta ||Y_n-X_n\beta||^2$.
\sskip 
Si on suppose par ailleurs que $\E(\epsilon_n)=0$ et $Cov(\epsilon_n)=\sigma^2I_n$, alors l'OLS est non biaisé et $Cov(\beta)=\sigma^2(X_n^TX_n)^{-1}$ et, si $n>p+1$, alors un estimateur de $\sigma^2$ est donné par:
$$\hat{\sigma}^2=\frac{||Y_n-X_n\hat \beta||^2}{n-p-1}$$
avec $\hat \beta = (X_n^TX_n)^{-1}X_n^TY_n$ qui défini alors l'OLS.
\sskip 
Ensuite, si on suppose que $\ind \epsilon n$ sont des VAs gaussiennes indépendantes centrées de variance $\sigma^2$, alors:
\smallskip 

\Prop 
Les estimateurs $\hat \beta$ et $\hat \sigma^2$ sont indépendants et: 
$$\hat \beta\sim \mc N_{p+1}(\beta, \sigma^2(X_n^TX_n)^{-1}) \ \ \text{et} \ \ (n-p-1)\frac{\hat \sigma^2}{\sigma^2}\sim \chi_2(n-p-1).$$
\bigskip 

\section{Intervalles de Confiance}
\subsection{Définitions Générales}
Soit $\alpha\in (0,1/2)$ la précision désirée pour notre intervalle de confiance.
\bigskip 

\Def \ Intervalle de confiance
\sskip 
Un intervalle de confiance de niveau $1-\alpha$ pour la QI $g(\theta)$ est un intervalle $I_n=[I_n^-,I_n^+]$ tel que $I_n^-$ et $I_n^+$ soient des statistiques et, pour tout $\theta \in \Theta$, $\Prob_\theta (g(\theta)\in I_n)=1-\alpha$.
\bskip 
Il peut être assez difficile, voire impossible, de construire des intervalles de confiance comme définis précédemment (on dit qu'ils sont exacts), on introduit donc les définitions suivantes:
\smallskip 

\Def 
\sskip 
Soit un intervalle $I_n$ tel que $I_n^-$ et $I_n^+$ soient des statistiques, alors cet intervalle est dit:
\begin{itemize}
    \item de confiance asymptotique si, pour tout $\theta\in\Theta$, $\lim_n \Prob_\theta (g(\theta)\in I_n)=1-\alpha$
    \item de confiance par excès si, pour tout $\theta\in\Theta$, $\lim_n \Prob_\theta (g(\theta)\in I_n)\ge 1-\alpha$.
\end{itemize}
\bigskip 

\subsection{Construction d'Intervalles de Confiance Exact}

\subsubsection{Fonction Pivot}
On commence par une définition générale:
\smallskip 

\Def \ VA libre
\sskip 
Un VA $Q$ est dite libre selon $\Prob_\theta$ si sa loi ne dépend pas de $\theta$.
\bigskip 

\Def \ Fonction pivot
\sskip 
Un fonction pivot pour $g(\theta)$ est une fonction $\pi_n:E^n\times g(\Theta)\to \R$ telle que $\pi_n(X_n,g(\theta))$ est libre.
\bigskip 

\subsubsection{Exemple dans le Modèle Gaussien}
On s'intéresse à la moyenne $\mu$ dans un modèle Gaussien qu'on estime classiquement avec $\bar X_n$. La loi de $\bar X_n$ selon $\Prob_{\mu,\sigma^2}$ est $\mc N(\mu,\sigma^2/n)$. Il vient alors:
$$Y_n=\frac{\bar X_n -\mu}{\sqrt{\sigma^2/n}}\sim \mc N(0,1) \ \ \text{est libre.}$$
Néanmoins, la fonction:
$$\pi_n(x_n,\mu)=\frac{\bar x_n -\mu}{\sqrt{\sigma^2/n}}$$
n'est pas une fonction pivot puisqu'elle dépend de $(\mu,\sigma^2)$ à travers $\mu$ et $\sigma$ et non uniquement de $\mu$.
\sskip 
Supposons donc momentanément que $\sigma^2$ soit connu, alors $\pi_n$ devient une fonction pivot. Il vient alors, pour tout réels a et b tels que $a<b$:
$$\Prob_{\mu,\sigma^2}(Y_n\in[a,b])=\frac{1}{\sqrt{2\pi}}\int_a^b\exp(-x^2/2)dx$$
donc, pour tout choix de a et b tels que:
$$\frac{1}{\sqrt{2\pi}}\int_a^b\exp(-x^2/2)dx=1-\alpha$$
alors l'intervalle $[\bar X_n - b\sqrt{\sigma^2/n},\bar X_n - a\sqrt{\sigma^2/n}]$ est un intervalle de confiance exact de précision $1-\alpha$ pour $\mu$.
\sskip
Rappelons par ailleurs que l'on définit par $\phi_r$ le quantile d'ordre $r$ de la distribution gaussienne standard, alors a et b permettent de satisfaire l'égalité attendue si et seulement si:
$$\exists r\in [0,\alpha]:\ a=\phi_r, \ b=\phi_{r+1-\alpha}.$$
Pour une telle paire $(a,b)$ et l'intervalle de confiance exact $[\bar X_n - b\sqrt{\sigma^2/n},\bar X_n - a\sqrt{\sigma^2/n}]$, la probabilité de sous-estimer $\mu$ est $r$, celle de la sur-estimer est $\alpha-r$.
\bskip 
Supposons désormais que $\sigma^2$ ne soit pas connu. Une idée classique consiste alors à remplace $\sigma^2$ par l'estimateur non-biaisé de la variance $S_n^2$ et on considère:
$$Y_n'=\frac{\bar X_n -\mu}{\sqrt{S_n^2/n}}\sim t(n-1) \ \ \text{est libre.}$$
Il vient alors que:
$$\pi_n(x_n,\mu)=\frac{\bar x_n -\mu}{\sqrt{s_n^2/n}} \ \ \text{avec} \ \ s_n^2=\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar x_i)^2$$
est une fonction pivot.
\sskip 
En conséquence, pour tout réels a et b tels que $a<b$, il vient:
$$\Prob_{\mu,\sigma^2}(Y_n'\in[a,b])=\int_a^bp_{n-1}(x)dx \ \ \text{avec} \ \ p_{n-1} \ \ \text{la densité de la loi} \ \ t(n-1).$$
Une nouvelle fois, dès que le couple $(a,b)$ vérifie:
$$\int_a^bp_{n-1}(x)dx=1-\alpha$$
on obtient un intervalle de confiance de précision 1-$\alpha$ pour $\mu$.
\bigskip 

\subsubsection{Résumé de la Méthode}
On commence par trouver une fonction pivot $Q_n=\pi_n(x_n, g(\theta))$, on réécrit la condition $Q_n\in[a,b]$ comme $g(\theta)\in I_n$ où les extrémités de $I_n$ sont des statistiques. Enfin on choisit un couple $(a,b)$ qui satisfait: $\Prob(Q_n\in [a,b])=1-\alpha$, ce qui revient à choisir $a=q_{n,r}$ et $b=q_{n,r+1-\alpha}$ avec $0\le r \le \alpha$ et $q_{n,r}$ le quantile d'ordre $r$ de $Q_n$.
\bigskip 

\subsection{Construction d'Intervalle de Confiance Asymptotique}
On rappelle que $\phi_r$ définit le quantile d'ordre $r$ de la distribution gaussienne standard.
\bigskip 

\Prop \ Intervalle de confiance asymptotique
\sskip 
Soit $Z_n$ un estimateur consistant et convergent normalement de $g(\theta)$, on note $V(\theta)$ sa variance asymptotique. On suppose qu'un estimateur consistant $\hat V_n$ de $V(\theta)$ est connu, alors:
$$\forall \alpha \in (0,1/2), \ I_n=[Z_n-\phi_{1-\alpha/2}\sqrt{\frac{\hat V_n}{n}},Z_n+\phi_{1-\alpha/2}\sqrt{\frac{\hat V_n}{n}}]$$
est un intervalle de confiance asymptotique avec une précision de $1-\alpha$ pour $g(\theta)$.
\bskip 
En général il n'est pas difficile de trouver un estimateur consistant de $V(\theta)$, dès que $V$ est continue et que $\hat \theta_n$ est un estimateur consistant de $\theta$, alors on a simplement: $\hat V_n=V(\hat \theta_n)$.
\bigskip 

\subsection{Construction d'Intervalle de Confiance par Excès}
On s'intéresse au cas où on ne possède pas de fonction pivot, comme dans le cas d'une loi de Bernoulli. On va alors construire des intervalles de confiance par excès à l'aide des inégalités de concentration.
\bigskip 

\Def \ Inégalité de concentration
\sskip 
Une inégalité de concentration pour une variable aléatoire $Y$ est une inégalité de la forme: 
$\Prob(|Y-\E(Y)|\ge r)\le c_Y(r)$ pour une fonction de concentration $c_Y$ convergent asymptotiquement vers 0.
\bskip 
Si $Z_n$ est un estimateur non-biaisé de $g(\theta)$ et qui vérifie une inégalité de concentration telle que:
$$\forall r>0, \ \sup_\theta \Prob_\theta (|Z_n-g(\theta)|\ge r)\le c_{Z_n}(r)$$
alors tout $r_{n,\alpha}>0$ tel que $c_{Z_n}(r_{n,\alpha})\le\alpha$ produit l'intervalle de confinace par excès: $[Z_n-r_{n,\alpha}, Z_n +r_{n,\alpha}]$ pour $g(\theta)$.
\bigskip 
\subsubsection{L'Inégalité de Bienaymé-Chebychev}
C'est l'inégalité classique:
$$\Prob( |Y-\E(Y)|\ge a)\le \frac{V(Y)}{a^2}.$$
Pour un modèle de Benoulli dans lequel on utilise $\bar X_n$ comme estimateur de $p$, alors, pour tout $a>0$, il vient:
$$\Prob_p(|\bar X_n - p|\ge a)\le \frac{p(1-p)}{a^2}$$
donc, pour $a$ tel que: $\frac{p(1-p)}{a^2}\le \alpha$, il vient:
$$\Prob_p(p\in[\bar X_n - a/\sqrt{n},\bar X_n + a/\sqrt{n} ])\le 1-\alpha.$$
Il reste à trouver une telle valeur de $a$ qui ne dépende pas de p, pour les VAs bornées, on peut utiliser le lemme suivant:
\smallskip 

\underline{Lemme:} Borne universelle de la variance
\sskip 
Soit Y une VA à valeurs dans $[0,1]$, alors: $V(Y)\le \frac{1}{4}$.
\bskip 
Il suffit alors de prendre $a=\frac{1}{2\sqrt{\alpha}}$.
\bigskip 
\subsubsection{Inégalité de Hoeffding}
On commence par introduire le lemme suivant:
\smallskip 

\underline{Lemme:} Inégalité de Hoeffding
\sskip 
Soient $\ind X n$ des VAs iid à valeurs dans $[0,1]$, alors, pour tout $n\ge 1$ et $r>0$, il vient:
$$\Prob(\sum_{i=1}^n(X_i-\E(X_i))\ge r\sqrt{n})\le \exp(-2r^2).$$
\sskip 
Il vient alors:
$$\Prob(|\bar X_n-\E(X_1)|\ge r/\sqrt{n})\le 2\exp(-2r^2).$$
\bigskip

\section{Estimateur du Maximum de Vraisemblance}
Jusqu'à présent, on a vu des méthodes d'estimation, de calcul d'intervalle de confiance qui reposent sur le choix de fonctions particulières. A l'inverse l'estimateur du maximum de vraisemblance permet d'estimer les paramètres d'un modèle de façon plus générale.
\subsection{Vraisemblance d'un Echantillon et Estimateur}
On s'intéresse à $(\ind X n)$ des VAs iid dans $\R^n$.
\bigskip 

\Def \ Vraisemblance d'une observation
\sskip 
Soit $x^*_n=(\ind x n)$ une valeur possible de $X^*_n=(\ind X n)$, la vraisemblance de cette observation est donnée par la fonction: 
$$L_n(x^*_n, \cdot) = \theta \mapsto \prod_{i=1}^np(x_i,\theta) \ \ \text{avec} \ \ p(x_i,\theta)= \Prob_\theta(X_i=x_i).$$
\bskip 
Dans le cas discret, alors: $L_n(x_n^*,\theta)= \Prob_\theta(X_1=x_1,...,X_n=x_n)$, si $\Prob_\theta$ admet une densité selon la mesure de Lebesgue, alors elle est donnée par: $x_n^*\mapsto L_n(x_n^*,\theta)$.
\bskip 
L'estimation du maximum de vraisemblance consiste à chercher le paramètre $\theta^*$ qui rend les valeurs observées les plus probables.
\smallskip 

\Def \ Estimateur du maximum de vraisemblance 
\sskip 
Supposons que, pour tout $x_n^*$, $\theta\mapsto L_n(x_n^*, \theta)$ atteigne un maximum global en $\theta^* = \theta_n(x_n^*)$. L'estimateur du maximum de vraisemblance (MLE) de $\theta$ est la statistique:
$$\hat{\theta_n} =\theta_n(X_n^*).$$
Dans le cas où il y a plusieur maximum, on peut simplement prendre $\hat{\theta_n}\in\arg\max_\theta L_n(x_n^*,\theta)$.
\bskip 
Si la fonction de vraisemblance est différentiable, on peut alors calculer $\hat{\theta_n}$ en cherchant les zéros du gradient. Dans cette perspective, il peut être intéressant de considérer la dérivée du logarithme de la vraisemblance:
$$l_n(x_n^*,\theta) = \log (L_n(x_n^*,\theta)).$$
\bigskip 

\subsection{Exemples}
\subsubsection{Modèle de Bernoulli}
On a alors: 
$$L_n(x_n^*, p )=\prod_{i=1}^n \Prob_p(X_i=x_i) = \prod_{i=1}^n p^{x_i}(1-p)^{1-x_i}$$
donc: $$l_n(x_n^*,p)=\sum_{i=1}^n(x_i\log(p)+(1-x_i)\log(1-p))=n\bar X_n\log(p) + n(1-\bar X_n)\log(1-p).$$
On peut alors calculer: 
$$\frac{\partial l}{\partial p}(x_n^*,p^*) = 0 \Longleftrightarrow (1-p^*)\bar X_n - p^* (1-\bar X_n) = 0 \Longleftrightarrow p^* =\bar X_n.$$
Il reste alors à s'assurer que $p^*$ correspond bien à un maximum, dans le cas du modèle de Bernoulli c'est bien le cas.
\bigskip 

\subsubsection{Cas du Modèle Gaussien}
On a alors:
$$L_n(x_n^*,\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}^n}\prod_{i=1}^n\exp(\frac{-(x_i-\mu)^2}{2\sigma^2})$$
donc: 
$$\frac{\partial l_n}{\partial \mu}(x_n^*,\mu^*,{\sigma^2}^*) = \frac{1}{{\sigma^2}^2}\sum_{i=1}^n(x_i - \mu^*) \ \ \text{et}\ \ \frac{\partial l_n}{\partial \sigma^2}(x_n^*,\mu^*,{\sigma^2}^*) = -\frac{n}{2\pi\sigma^*}+\sum_{i=1}^n\frac{(x_i-\mu^*)^2}{{\sigma^3}^*}.$$
On trouve alors : $\mu^*=\bar X_n$ et ${\sigma^2}^*=S_n^2$ qui sont les estimateurs qu'on trouve également avec la méthode des moments.
\bigskip 

\subsection{Optimalité du MLE}
Lorsqu'on a plusieurs estimateurs du même paramètre on peut comparer leur MSE, voire leur variance asymptotique s'ils sont asymptotiquements normaux. Le MLE est asymptotiquement normal, sa variance asymptotique est par ailleurs optimale dans les modèles réguliers.
\subsubsection{Modèle Régulier et Information de Fisher}
On rappelle que $\nabla\phi_\theta = (\frac{\partial \phi}{\partial\theta_1},...,\frac{\partial \phi}{\partial\theta_d})$.
\bigskip 

\Def \ Modèle régulier 
\sskip 
Un modèle paramétrique est régulier si: $\Theta$ est ouvert et:
\begin{itemize}
\item $\forall x_1,\forall \theta\in\Theta, \ L_1(x_1,\theta)>0$
\item $\forall x_1, \ \theta\mapsto l_1(x_1,\theta)\in \mc C^1(\Theta)$
\item $\forall \theta\in \Theta, \ \E_\theta(||\nabla_\theta l_1(X_1,\theta)||^2)<\pinf.$
\end{itemize}
\bigskip 

\Def \ Score dans un modèle régulier 
\sskip 
Dans un modèle régulier, on appelle score le vecteur aléatoire $\nabla_\theta l_1(X_1,\theta) = (\frac{1}{p(X_1,\theta)}\frac{\partial p}{\partial\theta_i}(X_1,\theta))_i$.
\sskip 
L'information de Fisher $I(\theta)$ d'un modèle régulier est: 
$$I(\theta) = Cov_\theta(\nabla_\theta l_1(X_1,\theta)).$$

\Prop 
\sskip 
On a, pour tout $\theta$, $\E_\theta(\nabla_\theta l_1(X_1,\theta)) = 0$ et les coefficients de $I(\theta)$ sont:
$$I_{i,j}(\theta) = \E_\theta(\frac{\partial l_1}{\partial \theta_i}(X_1,\theta)\frac{\partial l_1}{\partial \theta_j}(X_1,\theta)) = -\E_\theta(\frac{\partial^2l_1}{\partial\theta_i\partial\theta_j}(X_1,\theta)).$$
\bigskip 

\subsubsection{Estimateur Efficace}
On peut utiliser l'information de Fisher pour définir une notion d'efficacité pour les modèles non-biaisés.
\smallskip 

\Theo \ Borne de Cramér-Rao 
\sskip 
Pour un modèle régulier tel que $I(\theta)>0$ pour tout $\theta$, on note $\tilde \theta_n = t_n(X_n)$ un estimateur non-biaisé de $\theta$ avec la matrice de covariance $K_n(\theta)= Cov_\theta(\tilde \theta_n)$; alors:
$$K_n(\theta) \succeq \frac{I^{-1}(\theta)}{n}.$$
\bigskip 

\Def \ Estimateur efficace 
\sskip 
Un estimateur est dit efficace s'il est non-biaisé et si sa matrice de covariance vérifie $K_n(\theta) = \frac{I^{-1}(\theta)}{n}$.
\bskip 
On rappelle que si $\tilde \theta_n$ est un estimateur non-biaisé, alors: $MSE(\tilde \theta_n,\theta)=\V_\theta(\tilde\theta_n)=tr(K_n(\theta))\ge \frac{tr(I^{-1}(\theta))}{n}$. En conséquences, dans un modèle régulier, les estimateurs efficaces minimisent la MSE pour les estimateurs non-biaisés.
\bskip 
\section{Estimation Bayesienne}
Les techniques utilisées jusqu'à présent sont essentiellement motivées par le fait que les estimateurs sont consistants. Ainsi, si suffisament de données sont relevées, on peut retrouver la valeur réelle de la quantité d'intérêt, c'est l'approche fréquentiste.
\sskip 
Néanmoins, dans le certains cas, comme lorsque les données sont manquantes, il peut être intéressant d'avoir des connaissances, indépendantes des données observées, a priori sur la valeur de la quantité d'intérêt. C'est le principe des l'inférence bayésienne.
\bskip 
\subsection{Formalisation de l'Estimation Bayésienne}
On travaille toujours dans le cadre d'un modèle paramétrique $\mc P = \{\Prob_\theta; \ \theta\in\Theta\}$ où $\Theta$ est fini ou un sous-ensemble de $\R^q$.
\bskip 
La distribution a priori est une mesure de probabilité sur $\Theta$ qui évalue la crédibilité de chaque valeur de $\theta$ avant d'observer les données.
\bigskip 

\Def \ Distribution a posteriori 
\sskip 
Pour une distribution a priori $Q$, la distribution a posteriori est la mesure de probabilité $Q(\cdot | x_n^*)$ définie sur $\Theta$ par:
$$Q(d\theta | x_n^*) = \frac{L_n(x_n^*,\theta)Q(d\theta)}{\int_{\omega\in\Theta} L_n(x_n^*,\omega)Q(d\omega)},$$
où $L_n(x_n^*,\theta)$ est la vraisemblance de l'observation $x_n^*$.
\sskip 
Ainsi, pour tout sous-ensemble $B\subset \Theta$ mesurable, il vient:
$$Q(B | x_n^*) = \frac{\int_{\theta\in B}L_n(x_n^*,\theta)Q(d\theta)}{\int_{\omega\in\Theta} L_n(x_n^*,\omega)Q(d\omega)}.$$
\bskip 
Plus généralement, si $\Theta$ est dénombrable, alors:
$$Q(d\theta | x_n^*) = \frac{L_n(x_n^*,\theta)Q(\theta)}{\sum_{\omega\in\Theta} L_n(x_n^*,\omega)Q(\omega)},$$
si $Q$ a une densité $q$, alors $Q(\cdot, x_n^*)$ a la densité:
$$q(\theta| x_n^*)=\frac{L_n(x_n^*,\theta)q(\theta)}{\int_{\omega\in\Theta}L_n(x_n^*,\omega)q(\omega)d\omega}.$$
\bigskip 
La distributtion a posteriori doit être interprétée comme la crédibilité de $\theta$ étant donnée l'observation de $x_n^*$.
\bskip 
\subsection{Estimateur Bayesien}
La distribution a posteriori possède toutes les informations nécessaires pour l'étude de $\theta$. Toutefois, en tant que mesure de distribution de probabilité, elle n'est pas évidente à manipuler, il est donc intéressant d'introduire les notions d'estimateur (Bayesien) ou d'intervalle de confiance.
\bigskip 

\Def \ Moyenne postérieure (Sous l'hypothèse $\Theta\subset\R^q$ convexe)
\sskip 
La moyenne postérieure (PM) est la moyenne de la distribution a posteriori:
$$\widehat{\theta_n^{PM}} = \theta_n^{PM}(X_n), \ \ \text{avec} \ \ \theta_n^{PM}(x_n^*)= \int_{\theta\in\Theta}\theta Q(d\theta|x_n^*).$$
La VA $\widehat{\theta_n^{PM}}$ est une statistique et donc un estimateur de $\theta$ dans le sens fréquentiste.
\bskip 
Dans la définition suivante, on suppose que ou bien $\Theta$ est discret et on pose $q(\theta)=Q(\{\theta\})$ ou bien $Q$ a une densité $q$.
\smallskip 

\Def \ Maximum a posteriori (MAP)
\sskip 
Supposons que, pour tout $x_n^*\in E^n$, la fonction $\theta\mapsto q(\theta|x_n^*)$ possède un maximum unique atteint en $\theta = \theta_n^{MAP}(x_n^*)$, alors le MAP est défini par :
$\widehat{\theta_n^{MAP}} = \theta_n^{MAP}(X_n)$. Comme la PM, le MAP est un estimateur de $\theta$ dans le sens fréquentiel.
\bskip 
Il est généralement attendu que la distribution a posteriori se concentre autour de $\theta$ de façon asymptotique. Quand $\Theta$ est discrèt, la définition de ce phénomène est immédiate:
\smallskip 

\Def \ Consistence de la distribution a posteriori (cas discret)
\sskip 
L'estimateur Bayesien de $\theta$ avec la distribution a priori $Q$ est consistant si:
$$\forall \theta \in\Theta, \ \lim_{n\to\infty}Q(\theta|X_n)=1, \ \ \text{C } \Prob_\theta.$$
Pour le cas général, la définition est un peu plus technique:

\Def \ Consistence de la distribution a posteriori (cas $\Theta\subset\R^q$)
\sskip 
L'estimateur Bayesien de $\theta$ avec la distribution a priori $Q$ est consistant si:
$$\forall \theta \in\Theta, \ \forall\epsilon >0, \ \lim_{n\to\infty}Q(\{\omega\in\Theta | \ ||\omega - \theta||\ge\epsilon\}|X_n)=0, \ \ \text{presque sûrement selon } \Prob_\theta.$$

\Prop \ Condition suffisante de consistance
\sskip 
Pour tout $x_n^*\in E^n$, on définit la variance de la distribution a posteriori par:
$$V_n(x_n^*)=\int_{\theta\in\Theta}||\theta - \theta_n^{PM}(x_n^*)||^2Q(d\theta|x_n^*).$$
On peut alors vérifier que, si la PM $\widehat{\theta_n^{PM}}$ est fortement consistante et si $V_n(X_n)\longrightarrow 0$ presque sûrement selon $\Prob_\theta,$ pour tout $\theta$, alors l'estimateur bayésien de $\theta$, pour la distribution a priori $Q$, est consistant.
Dans le cas où $\Theta\subset\R^q$, on peut alors introduire le concept d'intervalle de crédibilité de niveau $1-\alpha$ qui définit un intervalle $I$ dont les bornes sont des statistiques telles que:
$$Q(I|X_n)=1-\alpha.$$
Ces intervalles sont l'équivalent Bayesien des intervalles de confiances.

\end{document}