\documentclass{article}

\title{Statistiques et Analyse de Données}
\author{Paul Lehaut}
\usepackage{mesraccourcis}

\begin{document}
\maketitle
\newpage
\tableofcontents
\newpage
\section{Rappels}
\subsection{Espérance et (Co)Variance}
On considère dans cette section des VA$\R$.
\bigskip

\Def 
\sskip 
L'espérance de $X\sim\Prob$ est l'intégral de Lebesgue:
$$\E (X):=\int_{\omega\in\Omega}X(\omega)d\Prob (\omega)=\int_{x\in\R}xd\Prob(x)$$
qui est bien définie si X est intégrable, c'est-à-dire si $\E(|X|)<+\infty$.
\sskip 
Si par ailleurs $\E(X^2)<+\infty$, alors la variance de $X$ est bien définie par:
$$\V (X):=\E((X-\E(X))^2)=\E(X^2)-\E(X)^2.$$
\bskip
L'inégalité de Jensen donne alors, pour X une VA$\R^d$ intégrable, soit $f$ une fonction à valeurs réelles convexe définie sur $\R^d$, alors $\E(f(X))$ est bien définie et:
$$f(\E(X))\le \E(f(X)).$$
\bigskip

\Def 
\sskip 
La covariance entre $X$ et $Y$, telles que $\E(X)<+\infty$ et $\E(Y)<+\infty$, est définie par:
$$Cov(X,Y):=\E((X-\E(X))(Y-\E(Y)))=\E(XY)-\E(X)\E(Y)$$
on a alors l'identité remarquable suivante:
$$\V (X+Y)=\V(X)+2Cov(X,Y)+\V(Y).$$
\bskip 
Lorsque $X$ est une VA$\R^d$ de carré intégrable, on peut définir sa matrice de covariance qui est symétrique positive.
\bigskip

\Def 
Le coefficient de corrélation de $X$ et $Y$ est définie par:
$$\rho_{X,Y} :=
\left\{
\begin{array}{ll}
\dfrac{\operatorname{Cov}(X,Y)}{\sqrt{\operatorname{Var}(X)} \, \sqrt{\operatorname{Var}(Y)}} \in [-1,1] & \text{si } \operatorname{Var}(X)\operatorname{Var}(Y) > 0, \\[1.5ex]
0 & \text{sinon.}
\end{array}
\right.$$
\bigskip
\subsection{Indépendance}

\Def 
Une famille finie de variables aléatoires $(X_1,...,X_n)$ définies sur $(\Omega, \mathcal{F})$ est indépendante si pour toute famille de sous-ensembles mesurables $(C_1,...,C_n)$ on a:
$$\Prob(X_1\in C_1,...,X_n\in C_n)=\Prob(X_1\in C_1)...\Prob(X_n\in C_n)$$
ou, de façon équivalente, si:
$$\Prob =\Prob_{X_1}\otimes...\otimes\Prob_{X_n}.$$
\bskip 
Si $\indn{X}$ sont indépendantes alors, pour toutes fonctions mesurables $\indn{f}$ telles que $f_1(X_1),...,f_n(X_n)$ soient intégrables, on a:
$$\E(f_1(X_1),...,f_n(X_n))=\E(f_1(X_1))...\E(f_n(X_n)).$$
\bigskip

\subsection{Variables Aléatoires Discrètes}
Si E est discret, toutes mesures de probabilités est caractérisée par la famille de nombres ($p(x), \ x\in E$). Les variables aléatoires principales sont définies dans le polycopié.
\bigskip

\subsection{Variables Aléatoires à Densité}
Les intégrales sont ici à considérer au sens de Lebesgue.
\bigskip

\Def 
\sskip
Une variable aléatoire $X$ est dite à densité p si:
$$\forall C \in \mc{B}(\R^d), \Prob(X\in C)=\int_{x\in C}p(x)dx$$
une fonction mesurable et positive p est une densité de probabilité si et seulement si:
$$\int_{x\in \R^d}p(x)dx=1.$$
\bskip
Des variables aléatoires $\indn{X}$ sont indépendantes si le vecteur aléatoire $(\indn{X})$ a pour densité
\newline
$p=p_{X_1}\otimes...\otimes p_{X_n}$.
\bigskip

\Theo \ Formule de transfert
\sskip 
Si X a une densité $p$, alors pour toutes fonctions mesurables $f$ telle que $\E(|f(X)|)<\pinf$, alors:
$$\E(f(X))=\int_{x\in \R^d}f(x)p(x)dx.$$
\bskip
Les exemples principaux de variables aléatoires à densité se trouvent dans le polycopié.
\bigskip

\subsection{Somme de VAs indépendantes}
Soient $X$ et $Y$ deux VAs indépendantes, on note $Z:=X+Y$.
\bigskip

\Prop 
\sskip
La densité de Z est:
$$r(z)=\int_{x\in\R^d}p_X(x)p_{Y}(z-x)dx$$
on l'appelle la convolution de $p_X$ et $p_Y$.
\bigskip

\subsection{Fonction de Répartition}
On se place dans le cas où E=$\R$.
\bigskip

\Def 
\sskip
La fonction de répartition d'un variable aléatoire $X$ est définie par:
$$\forall x\in \R, \ F(x)=\Prob(X\le x)$$
alors, pour tout réel $r\in (0,1)$, un quantile d'ordre r pour $X$ est un nombre $q_r$ tel que:
$$\Prob(X\le q_r)=F(q_r)=r.$$
\bskip 
En général un quantile n'existe pas toujours ou n'est pas unique, néanmoins, lorsque $X$ possède une densité qui est positive alors le quantile existe et est unique.
\bigskip

\Prop 
\sskip
Si $X$ est une VA$\R$ à densité $p$, alors sa fonction de répartition est continue et dérivable \pp, sa dérivée est \pp \ égale à sa fonction de densité.
\bigskip

\subsection{Fonction Charactéristique}
Soient $X$ et $Y$ des vecteurs aléatoires.
\bigskip

\Def \ Fonction Charactéristique
\sskip 
La fonction charactéristique de X est la fonction $\Psi_X : \R^d\longrightarrow \C$ définie par:
$$\Psi_X(u):=\E(e^{i<u,X>})=\E(\cos(<u,X>))+i\E(\sin(<u,X>)).$$
\bigskip 

\Prop 
\sskip
Si, pour tout $u\in\R^d$, $\Phi_X(u)=\Phi_Y(u)$, alors $X$ et $Y$ sont de même loi.
\bigskip

\Prop 
\sskip
Si $X\sim \mc{N}(\mu,\sigma^2)$ et $Y\sim \mc{N}(\nu,\tau ^2)$ sont indépendantes, alors:
$$X+Y\sim \mc{N}(\mu+\nu,\sigma^2+\tau^2).$$
\bigskip

\subsection{Vecteurs Gaussiens}
Soit $X$ un vecteur aléatoire.
\bigskip

\Def \ Vecteur Gaussien
\sskip 
$X$ est un vecteur Gaussien si, pour tout vecteur $u$, la variable aléatoire $<u,X>$ est gaussienne.
\bskip 
De ce qui précède, on déduit que, en notant $\E(X)=m$ et $Cov(X)=K$, alors:
$$<u,X>\sim\mc{N}(<u,m>,<u,Ku>) \ \ \text{et } \ \Phi_X(u)=\exp(i<u,m>-\frac{1}{2}<u,Ku>).$$
Par ailleurs, si $K$ est inversible, alors $X$ a la densité:
$$\frac{1}{\sqrt{(2\pi)^d\det K}}\exp(-\frac{1}{2}<x-m,K^{-1}(x-m)>)$$
sinon $X$ n'a pas de densité.
\bigskip

\subsection{Théorèmes de Convergence}
Soient $(X_n)$ et $X$ des vecteurs aléatoires.
\bigskip

\Def
\sskip
$(X_n)$ converge vers $X$ presque sûrement si: $\Prob(\lim X_n =X)=1$.
\sskip 
$(X_n)$ converge vers $X$ en probabilité si, pour tout $\epsilon>0, \ \lim\Prob(||X_n-X||\ge \epsilon)=0$.
\sskip 
$(X_n)$ converge vers $X$ en distribution si, pour toute fonction continue et bornée $f:\R^d\longrightarrow \R $, $\E(f(X_n))$ converge vers $\E(f(X))$.
\bigskip 

\Theo \ Convergence Dominée
\sskip 
Supposons que $(X_n)$ converge vers $X$ presque sûrement et qu'il existe $Y$ positive et intégrable telle que: $||X_n||\le Y$ presque sûrement, alors $\E(X_n)$ converge vers $\E(X)$.
\bskip 
Pour $f$ une fonction continue, si $(X_n)$ converge presque sûrement (respectivement en probabilité ou en distribution) vers $X$ alors $f(X_n)$ converge presque sûrement vers $f(X)$ (respectivement en probabilité ou en distribution).
\bigskip

\Prop 
\sskip 
$(X_n)$ converge en distribution (c'est-à-dire en loi) vers $X$ si et seulement si $\Psi_{X_n}(u)$ converge vers $\Psi_X(u)$ pour tout u.
\sskip
La convergence presque sûre implique la converge en probabilité qui implique elle même la converge en distribution.
\bskip
Si on se place dans le cadre réel, alors on a les équivalences suivantes:

-$(X_n)$ converge vers $X$ en distribution

-$\Prob (X_n\le x)\longrightarrow \Prob(X\le x)$ pour tout $x$ tel que $\Prob(X=x)=0$

-$\Prob (X_n< x)\longrightarrow \Prob(X< x)$ pour tout $x$ tel que $\Prob(X=x)=0$ 
\bskip 
On dit que la suite $(X_n)$ est indépendante et indentiquement distribuée si ses variables sont indépendantes et de même loi. On note alors:
$$\bar{X_n}=\frac{1}{n}\sum_{i=1}^nX_i$$
la moyenne empirique de $\indn X$.
\bigskip 

\Theo \ La Loi Forte des Grands Nombres
\sskip
Soit $(X_n)$ une suite de VA$\R^d$ iid telle que $\E(||X_1||)<\pinf$, alors:
$$\lim \bar{X_n}=\E(X_1) \ \ \text{presque sûrement}.$$
\bigskip

\Theo \ Théorème Central Limite Multivarié
\sskip 
Soit $(X_n)$ une suite de VA$\R^d$ iid telle que $\E(||X_1||^2)<\pinf$, alors:
$$\lim\sqrt{n}(\bar{X_n}-\E(X_1))=\mc{N}(0,Cov(X_1)) \ \ \text{en distribution.}$$
\newpage
\section{Estimateur dans un Modèle Paramétrique}
On considère un échantillon $\indn{X}$ de VA iid dans un espace mesurable $(E,\mc E)$ de loi $\Prob$ inconnue qu'on cherche à éclaircir. On cherche par exemple à estimer $\E_\Prob(X_1), \ V_\Prob(X_1)$ l'histogramme de $\Prob$ ou encore d'autre quantité d'intérêt (QI).
\bskip
On se restreint à des lois d'une certaine forme caractérisées par un ou certain paramètres. Formellement, on considère une famille de lois: $\{\Prob_\theta; \ \theta \in \Theta\}$.
\bigskip 

\subsection{Estimateurs}
On cherche donc à estimer une QI à l'aide de notre échantillon de VA, pour ce faire on va chercher à approcher QI par une fonction de l'échantillon appelée statistique.
\bigskip 

\Def 
\sskip 
Une statistique $T_n$ est une VA de la forme $T_n=t_n(\indn X)$ avec $t_n$ déterministe et qui ne dépend pas de $\Prob$.
\sskip
On appelle un estimateur une statistique qui vise à approcher une certaine QI.
\bigskip 

\Def 
\sskip 
Un estimateur $Z_n$ d'une QI est dit constistant si $Z_n$ converge en probabilité vers QI, il est fortement consistant s'il converge vers QI \ps.
\bskip 
Par exemple, dans le cas où $E=\R$, alors la moyenne empirique $\bar X_n=\frac 1 n\sum_{i=1}^nX_i$ est un estimateur fortement consistant de $\E(X_1)$.
\bigskip

\subsection{Biais et Risque Quadratique}
Le biais et la MSE permettent de quantifier la distance d'un estimateur à sa QI dans un régime non asymptotique (ie n est fini).
\bigskip 

\Def 
\sskip 
Soit un estimateur intégrable $Z_n$, le biais de $Z_n$ est:
$$b(Z_n)=\E(Z_n)-QI, \ \ \text{il s'agit de la distance moyenne de l'estimateur à la QI}$$
si ce biais est nul, on dit que l'estimateur est non biaisé.
\bskip 
Il est intéressant de remarquer que la variance empirique est biaisée ce qui motive la définition suivante:
\bigskip 

\Def 
\sskip 
Si $E=\R$, l'estimateur non-biaisé de la variance est:
$$S_n^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar X_i)^2.$$
\bskip 
Une mesure plus précise de la distance de l'estimateur à la QI peut être donnée par la MSE.
\bigskip 

\Def 
\sskip 
Soit $Z_n$ un estimateur de carré intégrable, la MSE de $Z_n$ est définie par:
$$MSE(Z_n)=E(||Z_n-QI||^2).$$
\bigskip 

\Prop 
\sskip 
On peut a l'égalité suivante:
$$MSE(Z_n)=||b(Z_n)||^2+V(Z_n).$$
\bskip 
En général on ne peut pas minimiser à la fois la variance et le biais. En data science il peut être intéressant d'introduire un biais pour réduire la variance du modèle et donc le risque d'overfitting.
\bigskip 

\subsection{Modèle Paramétrique et Estimation du Moment}
On s'intéresse ici à l'estimation de la distribution complète $\Prob$ de $X_1$. Il y a deux approches principales: la méthode non paramétrique (par histogramme) et la méthode paramétrique qui repose sur la supposition que $\Prob$ a une certaine forme (comme exponentielle ou gaussienne).
\bigskip 

\Def 
\sskip 
Un modèle paramétrique sur E est un ensemble de mesures de probabilités:
$$\mc P=\{\Prob_\theta; \ \theta\in\Theta\}$$
sur l'espace E, indexé par un ensemble de paramètres $\Theta\subset\R^k$.
\bskip 
Il est important de constater que si, pour deux valeurs distinctes $\theta$ et $\theta'$, on a $\Prob_\theta = \Prob_{\theta'}$ alors il n'est pas possible de distinguer $\theta$ de $\theta'$ par simple observation de $\indn X$. On travaillera donc toujours en supposant que la fonction $\theta\mapsto\Prob_\theta$ est injective, on dira alors que $\mc P$ est identifiable.
\sskip 
On fixe désormais un modèle paramétrique $\mc P$. Pour tout $\theta$ il est pratique de dénoté par $\Prob_\theta$ la mesure de probabilité pour laquelle, pour tout $n\ge 1$, les VAs $\indn X$ sont iid selon $\Prob_\theta$. On définit de façon similaire $\E_\theta, \ V_\theta$ etc.
\bigskip 

\Prop 
\sskip
Soit X une VA$\R$ non déterministe intégrable et qui prend ses valeurs dans un intervalle I. Soit $\Phi : I\to \R$ strictement convexe, alors: $\Phi(\E(X))<\E(\Phi(X))$.
\bskip 
La méthode des moments est une procédure naturelle pour construire des estimateurs, on détaille ici l'exemple pour le modèle exponentielle: $\{\epsilon(\lambda); \ \lambda >0\}$ dans lequel on cherche un estimateur de $\lambda$:
\begin{itemize}
\item La loi forte des grands nombres donne, pour tout $\lambda>0$: $\bar X_n\to\E_\lambda(X_1)=\frac 1 \lambda$ \ps, donc $\frac{1}{\bar X_n}\to \lambda$ \ps.
\newline
La continuité de la fonction $x\mapsto \frac 1 x$ sur $\R_+^*$ assure donc que $\bar\lambda_n =\frac{1}{\bar X_n}$ est un estimateur fortement consistant de $\lambda$.
\end{itemize}
La généralisation abstraite de cette méthode s'énonce de la façon suivante:
\begin{itemize}
\item Pour l'estimation de $g(\theta)\in  \R^d$, cela consiste à trouver $\phi$ et $m$ des fonctions telles que:
$$\forall \theta\in \Theta, \ \E_\theta(\phi(X_1)) = m(g(\theta)).$$
Pour le modèle exponentielle on a pris: $\phi(x) =x, \ g(\lambda)=\lambda$ et $m(\lambda)=\frac{1}{\lambda}$.
\newline 
Alors, la loi forte des grands nombres nous permet donc d'approximer $m(g(\theta))$ par: $\frac{1}{n}\sum_{i=1}^n\phi(X_i)$ de sorte que, si $m$ possède une fonction réciproque continue $m^{-1}$, alors:
$$Z_n=m^{-1} (\frac{1}{n}\sum_{i=1}^n\phi(X_i))$$
est un estimateur fortement consistant de $g(\theta)$.
\end{itemize}
\bigskip 

\subsection{Convergence Normale}
La construction d'un estimateur par la méthode des moments dépend du choix arbitraire de la fonction $\phi$, donc différents choix de $\phi$ peuvent donner différents estimateurs qui sont tous, par construction, fortement consistant. Pour déterminer l'estimateur le plus intéressant en pratique, on peut s'intéresser à celui qui converge 'le plus vite' vers la QI. Cette vitesse de convergence peut être mesurée à l'aide de la notion de variance asymptotique.
\bigskip 

\Def 
\sskip 
Un estimateur consistant $Z_n$ de $g(\theta)$ est asymptotiquement normal si, pour tout $\theta$, il existe une matrice symétrique positive $K(\theta)\in \M_d(\R)$ telle que: $\sqrt{n}(Z_n-g(\theta))$ converge en distribution vers $\mc N_d(0,K(\theta))$.
\sskip 
La fonction $\theta\mapsto K(\theta)$ est appelé la covariance asymptotique de $Z_n$.
\bigskip 

\Theo \ Méthode delta
\sskip 
Soit $(\zeta_n)$ une suite de VA à valeurs dans $\R^k$ et $a\in \R^k$ telles que $\zeta_n\to a$ en probabilité et $\sqrt{n}(\zeta_n-a)$ converge en distribution vers un vecteur aléatoire $Y\in\R^k$. Soit $\mc U$ un ouvert de $\R^k$ qui contient $a$, soit $\Phi :\mc U\to \R^d$ de classe $\mc C^1$, alors:
$$\lim_{n\to\pinf}\sqrt{n}(\Phi(\zeta_n)-\Phi(a))=\nabla\Phi(a)Y$$
en distribution.
\bigskip 

\Theo \ Slutsky
\sskip 
Soit $((X_n,Y_n))$ une suite de couples de VAs telle que $(X_n)$ converge en probabilité vers une variable déterministe $a$ et que $(Y_n)$ converge en distribution vers une variable aléatoire $Y$. Alors $((X_n,Y_n))$ converge en distribution vers $(a,Y)$, et, par conséquent, pour toute fonction continue $\Psi$, $(\Psi(X_n,Y_n))$ converge en distribution vers $\Psi(a,Y)$.
\end{document}