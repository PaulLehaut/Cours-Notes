\documentclass{article}

\title{Statistiques et Analyse de Données}
\author{Paul Lehaut}
\usepackage{mesraccourcis}

\begin{document}
\maketitle
\newpage
\tableofcontents
\newpage
\section{Rappels}
\subsection{Espérance et (Co)Variance}
On considère dans cette section des VA$\R$.
\bigskip

\Def 
\sskip 
L'espérance de $X\sim\Prob$ est l'intégral de Lebesgue:
$$\E (X):=\int_{\omega\in\Omega}X(\omega)d\Prob (\omega)=\int_{x\in\R}xd\Prob(x)$$
qui est bien définie si X est intégrable, c'est-à-dire si $\E(|X|)<+\infty$.
\sskip 
Si par ailleurs $\E(X^2)<+\infty$, alors la variance de $X$ est bien définie par:
$$\V (X):=\E((X-\E(X))^2)=\E(X^2)-\E(X)^2.$$
\bskip
L'inégalité de Jensen donne alors, pour X une VA$\R^d$ intégrable, soit $f$ une fonction à valeurs réelles convexe définie sur $\R^d$, alors $\E(f(X))$ est bien définie et:
$$f(\E(X))\le \E(f(X)).$$
\bigskip

\Def 
\sskip 
La covariance entre $X$ et $Y$, telles que $\E(X)<+\infty$ et $\E(Y)<+\infty$, est définie par:
$$Cov(X,Y):=\E((X-\E(X))(Y-\E(Y)))=\E(XY)-\E(X)\E(Y)$$
on a alors l'identité remarquable suivante:
$$\V (X+Y)=\V(X)+2Cov(X,Y)+\V(Y).$$
\bskip 
Lorsque $X$ est une VA$\R^d$ de carré intégrable, on peut définir sa matrice de covariance qui est symétrique positive.
\bigskip

\Def 
Le coefficient de corrélation de $X$ et $Y$ est définie par:
$$\rho_{X,Y} :=
\left\{
\begin{array}{ll}
\dfrac{\operatorname{Cov}(X,Y)}{\sqrt{\operatorname{Var}(X)} \, \sqrt{\operatorname{Var}(Y)}} \in [-1,1] & \text{si } \operatorname{Var}(X)\operatorname{Var}(Y) > 0, \\[1.5ex]
0 & \text{sinon.}
\end{array}
\right.$$
\bigskip
\subsection{Indépendance}

\Def 
Une famille finie de variables aléatoires $(X_1,...,X_n)$ définies sur $(\Omega, \mathcal{F})$ est indépendante si pour toute famille de sous-ensembles mesurables $(C_1,...,C_n)$ on a:
$$\Prob(X_1\in C_1,...,X_n\in C_n)=\Prob(X_1\in C_1)...\Prob(X_n\in C_n)$$
ou, de façon équivalente, si:
$$\Prob =\Prob_{X_1}\otimes...\otimes\Prob_{X_n}.$$
\bskip 
Si $\indn{X}$ sont indépendantes alors, pour toutes fonctions mesurables $\indn{f}$ telles que $f_1(X_1),...,f_n(X_n)$ soient intégrables, on a:
$$\E(f_1(X_1),...,f_n(X_n))=\E(f_1(X_1))...\E(f_n(X_n)).$$
\bigskip

\subsection{Variables Aléatoires Discrètes}
Si E est discret, toutes mesures de probabilités est caractérisée par la famille de nombres ($p(x), \ x\in E$). Les variables aléatoires principales sont définies dans le polycopié.
\bigskip

\subsection{Variables Aléatoires à Densité}
Les intégrales sont ici à considérer au sens de Lebesgue.
\bigskip

\Def 
\sskip
Une variable aléatoire $X$ est dite à densité p si:
$$\forall C \in \mc{B}(\R^d), \Prob(X\in C)=\int_{x\in C}p(x)dx$$
une fonction mesurable et positive p est une densité de probabilité si et seulement si:
$$\int_{x\in \R^d}p(x)dx=1.$$
\bskip
Des variables aléatoires $\indn{X}$ sont indépendantes si le vecteur aléatoire $(\indn{X})$ a pour densité
\newline
$p=p_{X_1}\otimes...\otimes p_{X_n}$.
\bigskip

\Theo \ Formule de transfert
\sskip 
Si X a une densité $p$, alors pour toutes fonctions mesurables $f$ telle que $\E(|f(X)|)<\pinf$, alors:
$$\E(f(X))=\int_{x\in \R^d}f(x)p(x)dx.$$
\bskip
Les exemples principaux de variables aléatoires à densité se trouvent dans le polycopié.
\bigskip

\subsection{Somme de VAs indépendantes}
Soient $X$ et $Y$ deux VAs indépendantes, on note $Z:=X+Y$.
\bigskip

\Prop 
\sskip
La densité de Z est:
$$r(z)=\int_{x\in\R^d}p_X(x)p_{Y}(z-x)dx$$
on l'appelle la convolution de $p_X$ et $p_Y$.
\bigskip

\subsection{Fonction de Répartition}
On se place dans le cas où E=$\R$.
\bigskip

\Def 
\sskip
La fonction de répartition d'un variable aléatoire $X$ est définie par:
$$\forall x\in \R, \ F(x)=\Prob(X\le x)$$
alors, pour tout réel $r\in (0,1)$, un quantile d'ordre r pour $X$ est un nombre $q_r$ tel que:
$$\Prob(X\le q_r)=F(q_r)=r.$$
\bskip 
En général un quantile n'existe pas toujours ou n'est pas unique, néanmoins, lorsque $X$ possède une densité qui est positive alors le quantile existe et est unique.
\bigskip

\Prop 
\sskip
Si $X$ est une VA$\R$ à densité $p$, alors sa fonction de répartition est continue et dérivable \pp, sa dérivée est \pp \ égale à sa fonction de densité.
\bigskip

\subsection{Fonction Charactéristique}
Soient $X$ et $Y$ des vecteurs aléatoires.
\bigskip

\Def \ Fonction Charactéristique
\sskip 
La fonction charactéristique de X est la fonction $\Psi_X : \R^d\longrightarrow \C$ définie par:
$$\Psi_X(u):=\E(e^{i<u,X>})=\E(\cos(<u,X>))+i\E(\sin(<u,X>)).$$
\bigskip 

\Prop 
\sskip
Si, pour tout $u\in\R^d$, $\Phi_X(u)=\Phi_Y(u)$, alors $X$ et $Y$ sont de même loi.
\bigskip

\Prop 
\sskip
Si $X\sim \mc{N}(\mu,\sigma^2)$ et $Y\sim \mc{N}(\nu,\tau ^2)$ sont indépendantes, alors:
$$X+Y\sim \mc{N}(\mu+\nu,\sigma^2+\tau^2).$$
\bigskip

\subsection{Vecteurs Gaussiens}
Soit $X$ un vecteur aléatoire.
\bigskip

\Def \ Vecteur Gaussien
\sskip 
$X$ est un vecteur Gaussien si, pour tout vecteur $u$, la variable aléatoire $<u,X>$ est gaussienne.
\bskip 
De ce qui précède, on déduit que, en notant $\E(X)=m$ et $Cov(X)=K$, alors:
$$<u,X>\sim\mc{N}(<u,m>,<u,Ku>) \ \ \text{et } \ \Phi_X(u)=\exp(i<u,m>-\frac{1}{2}<u,Ku>).$$
Par ailleurs, si $K$ est inversible, alors $X$ a la densité:
$$\frac{1}{\sqrt{(2\pi)^d\det K}}\exp(-\frac{1}{2}<x-m,K^{-1}(x-m)>)$$
sinon $X$ n'a pas de densité.
\bigskip

\subsection{Théorèmes de Convergence}
Soient $(X_n)$ et $X$ des vecteurs aléatoires.
\bigskip

\Def
\sskip
$(X_n)$ converge vers $X$ presque sûrement si: $\Prob(\lim X_n =X)=1$.
\sskip 
$(X_n)$ converge vers $X$ en probabilité si, pour tout $\epsilon>0, \ \lim\Prob(||X_n-X||\ge \epsilon)=0$.
\sskip 
$(X_n)$ converge vers $X$ en distribution si, pour toute fonction continue et bornée $f:\R^d\longrightarrow \R $, $\E(f(X_n))$ converge vers $\E(f(X))$.
\bigskip 

\Theo \ Convergence Dominée
\sskip 
Supposons que $(X_n)$ converge vers $X$ presque sûrement et qu'il existe $Y$ positive et intégrable telle que: $||X_n||\le Y$ presque sûrement, alors $\E(X_n)$ converge vers $\E(X)$.
\bskip 
Pour $f$ une fonction continue, si $(X_n)$ converge presque sûrement (respectivement en probabilité ou en distribution) vers $X$ alors $f(X_n)$ converge presque sûrement vers $f(X)$ (respectivement en probabilité ou en distribution).
\bigskip

\Prop 
\sskip 
$(X_n)$ converge en distribution (c'est-à-dire en loi) vers $X$ si et seulement si $\Psi_{X_n}(u)$ converge vers $\Psi_X(u)$ pour tout u.
\sskip
La convergence presque sûre implique la converge en probabilité qui implique elle même la converge en distribution.
\bskip
Si on se place dans le cadre réel, alors on a les équivalences suivantes:

-$(X_n)$ converge vers $X$ en distribution

-$\Prob (X_n\le x)\longrightarrow \Prob(X\le x)$ pour tout $x$ tel que $\Prob(X=x)=0$

-$\Prob (X_n< x)\longrightarrow \Prob(X< x)$ pour tout $x$ tel que $\Prob(X=x)=0$ 
\bskip 
On dit que la suite $(X_n)$ est indépendante et indentiquement distribuée si ses variables sont indépendantes et de même loi. On note alors:
$$\bar{X_n}=\frac{1}{n}\sum_{i=1}^nX_i$$
la moyenne empirique de $\indn X$.
\bigskip 

\Theo \ La Loi Forte des Grands Nombres
\sskip
Soit $(X_n)$ une suite de VA$\R^d$ iid telle que $\E(||X_1||)<\pinf$, alors:
$$\lim \bar{X_n}=\E(X_1) \ \ \text{presque sûrement}.$$
\bigskip

\Theo \ Théorème Central Limite Multivarié
\sskip 
Soit $(X_n)$ une suite de VA$\R^d$ iid telle que $\E(||X_1||^2)<\pinf$, alors:
$$\lim\sqrt{n}(\frac{\bar{X_n}-\E(X_1)}{Cov(X_1)})=\mc{N}(0,1) \ \ \text{en distribution.}$$
\newpage
\section{Estimateur dans un Modèle Paramétrique}
On considère un échantillon $\indn{X}$ de VA iid dans un espace mesurable $(E,\mc E)$ de loi $\Prob$ inconnue qu'on cherche à éclaircir. On cherche par exemple à estimer $\E_\Prob(X_1), \ V_\Prob(X_1)$ l'histogramme de $\Prob$ ou encore d'autre quantité d'intérêt (QI).
\bskip
On se restreint à des lois d'une certaine forme caractérisées par un ou certain paramètres. Formellement, on considère une famille de lois: $\{\Prob_\theta; \ \theta \in \Theta\}$.
\bigskip 

\subsection{Estimateurs}
On cherche donc à estimer une QI à l'aide de notre échantillon de VA, pour ce faire on va chercher à approcher QI par une fonction de l'échantillon appelée statistique.
\bigskip 

\Def 
\sskip 
Une statistique $T_n$ est une VA de la forme $T_n=t_n(\indn X)$ avec $t_n$ déterministe et qui ne dépend pas de $\Prob$.
\sskip
On appelle un estimateur une statistique qui vise à approcher une certaine QI.
\bigskip 

\Def 
\sskip 
Un estimateur $Z_n$ d'une QI est dit constistant si $Z_n$ converge en probabilité vers QI, il est fortement consistant s'il converge vers QI \ps.
\bskip 
Par exemple, dans le cas où $E=\R$, alors la moyenne empirique $\bar X_n=\frac 1 n\sum_{i=1}^nX_i$ est un estimateur fortement consistant de $\E(X_1)$.
\bigskip

\subsection{Biais et Risque Quadratique}
Le biais et la MSE permettent de quantifier la distance d'un estimateur à sa QI dans un régime non asymptotique (ie n est fini).
\bigskip 

\Def 
\sskip 
Soit un estimateur intégrable $Z_n$, le biais de $Z_n$ est:
$$b(Z_n)=\E(Z_n)-QI, \ \ \text{il s'agit de la distance moyenne de l'estimateur à la QI}$$
si ce biais est nul, on dit que l'estimateur est non biaisé.
\bskip 
Il est intéressant de remarquer que la variance empirique est biaisée ce qui motive la définition suivante:
\bigskip 

\Def 
\sskip 
Si $E=\R$, l'estimateur non-biaisé de la variance est:
$$S_n^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar X_i)^2.$$
\bskip 
Une mesure plus précise de la distance de l'estimateur à la QI peut être donnée par la MSE.
\bigskip 

\Def 
\sskip 
Soit $Z_n$ un estimateur de carré intégrable, la MSE de $Z_n$ est définie par:
$$MSE(Z_n)=E(||Z_n-QI||^2).$$
\bigskip 

\Prop 
\sskip 
On peut a l'égalité suivante:
$$MSE(Z_n)=||b(Z_n)||^2+V(Z_n).$$
\bskip 
En général on ne peut pas minimiser à la fois la variance et le biais. En data science il peut être intéressant d'introduire un biais pour réduire la variance du modèle et donc le risque d'overfitting.
\bigskip 

\subsection{Modèle Paramétrique et Estimation du Moment}
On s'intéresse ici à l'estimation de la distribution complète $\Prob$ de $X_1$. Il y a deux approches principales: la méthode non paramétrique (par histogramme) et la méthode paramétrique qui repose sur la supposition que $\Prob$ a une certaine forme (comme exponentielle ou gaussienne).
\bigskip 

\Def 
\sskip 
Un modèle paramétrique sur E est un ensemble de mesures de probabilités:
$$\mc P=\{\Prob_\theta; \ \theta\in\Theta\}$$
sur l'espace E, indexé par un ensemble de paramètres $\Theta\subset\R^k$.
\bskip 
Il est important de constater que si, pour deux valeurs distinctes $\theta$ et $\theta'$, on a $\Prob_\theta = \Prob_{\theta'}$ alors il n'est pas possible de distinguer $\theta$ de $\theta'$ par simple observation de $\indn X$. On travaillera donc toujours en supposant que la fonction $\theta\mapsto\Prob_\theta$ est injective, on dira alors que $\mc P$ est identifiable.
\sskip 
On fixe désormais un modèle paramétrique $\mc P$. Pour tout $\theta$ il est pratique de dénoté par $\Prob_\theta$ la mesure de probabilité pour laquelle, pour tout $n\ge 1$, les VAs $\indn X$ sont iid selon $\Prob_\theta$. On définit de façon similaire $\E_\theta, \ V_\theta$ etc.
\bigskip 

\Prop 
\sskip
Soit X une VA$\R$ non déterministe intégrable et qui prend ses valeurs dans un intervalle I. Soit $\Phi : I\to \R$ strictement convexe, alors: $\Phi(\E(X))<\E(\Phi(X))$.
\bskip 
La méthode des moments est une procédure naturelle pour construire des estimateurs, on détaille ici l'exemple pour le modèle exponentielle: $\{\epsilon(\lambda); \ \lambda >0\}$ dans lequel on cherche un estimateur de $\lambda$:
\begin{itemize}
\item La loi forte des grands nombres donne, pour tout $\lambda>0$: $\bar X_n\to\E_\lambda(X_1)=\frac 1 \lambda$ \ps, donc $\frac{1}{\bar X_n}\to \lambda$ \ps.
\newline
La continuité de la fonction $x\mapsto \frac 1 x$ sur $\R_+^*$ assure donc que $\bar\lambda_n =\frac{1}{\bar X_n}$ est un estimateur fortement consistant de $\lambda$.
\end{itemize}
La généralisation abstraite de cette méthode s'énonce de la façon suivante:
\begin{itemize}
\item Pour l'estimation de $g(\theta)\in  \R^d$, cela consiste à trouver $\phi$ et $m$ des fonctions telles que:
$$\forall \theta\in \Theta, \ \E_\theta(\phi(X_1)) = m(g(\theta)).$$
Pour le modèle exponentielle on a pris: $\phi(x) =x, \ g(\lambda)=\lambda$ et $m(\lambda)=\frac{1}{\lambda}$.
\newline 
Alors, la loi forte des grands nombres nous permet donc d'approximer $m(g(\theta))$ par: $\frac{1}{n}\sum_{i=1}^n\phi(X_i)$ de sorte que, si $m$ possède une fonction réciproque continue $m^{-1}$, alors:
$$Z_n=m^{-1} (\frac{1}{n}\sum_{i=1}^n\phi(X_i))$$
est un estimateur fortement consistant de $g(\theta)$.
\end{itemize}
\bigskip 

\subsection{Convergence Normale}
La construction d'un estimateur par la méthode des moments dépend du choix arbitraire de la fonction $\phi$, donc différents choix de $\phi$ peuvent donner différents estimateurs qui sont tous, par construction, fortement consistant. Pour déterminer l'estimateur le plus intéressant en pratique, on peut s'intéresser à celui qui converge 'le plus vite' vers la QI. Cette vitesse de convergence peut être mesurée à l'aide de la notion de variance asymptotique.
\bigskip 

\Def 
\sskip 
Un estimateur consistant $Z_n$ de $g(\theta)$ est asymptotiquement normal si, pour tout $\theta$, il existe une matrice symétrique positive $K(\theta)\in \M_d(\R)$ telle que: $\sqrt{n}(Z_n-g(\theta))$ converge en distribution vers $\mc N_d(0,K(\theta))$.
\sskip 
La fonction $\theta\mapsto K(\theta)$ est appelé la covariance asymptotique de $Z_n$.
\bigskip 

\Theo \ Méthode delta
\sskip 
Soit $(\zeta_n)$ une suite de VA à valeurs dans $\R^k$ et $a\in \R^k$ telles que $\zeta_n\to a$ en probabilité et $\sqrt{n}(\zeta_n-a)$ converge en distribution vers un vecteur aléatoire $Y\in\R^k$. Soit $\mc U$ un ouvert de $\R^k$ qui contient $a$, soit $\Phi :\mc U\to \R^d$ de classe $\mc C^1$, alors:
$$\lim_{n\to\pinf}\sqrt{n}(\Phi(\zeta_n)-\Phi(a))=\nabla\Phi(a)Y$$
en distribution.
\bigskip 

\Theo \ Slutsky
\sskip 
Soit $((X_n,Y_n))$ une suite de couples de VAs telle que $(X_n)$ converge en probabilité vers une variable déterministe $a$ et que $(Y_n)$ converge en distribution vers une variable aléatoire $Y$. Alors $((X_n,Y_n))$ converge en distribution vers $(a,Y)$, et, par conséquent, pour toute fonction continue $\Psi$, $(\Psi(X_n,Y_n))$ converge en distribution vers $\Psi(a,Y)$.
\bigskip 

\section{Statistiques dans des Modèles Gaussiens}
On commence par quelques rappels.
\bigskip 

\Def 
\sskip 
Un vecteur aléatoire $G\in\R^n$ est un vecteur gaussien standard si chacune de ces composantes est de loi $\mc N(0,1)$ et si elles sont indépendantes.
\sskip 
La fonction caractéristique d'un vecteur gaussien standard est: $\psi_G(u)=e^{-\frac{||u||^2}{2}}$ (il s'agit du cas particulier de la fonction caractéristique d'un vecteur gaussien: $\phi_X(x)=\exp(it^Tm-\frac 1 2 t^T\Sigma t)$).
\bigskip 

\Theo \ Cochran 
\sskip 
Soit $G\sim \mc N_n(0,I_n)$, pour tout sous-\ev \ E de $\R^n$, les coordonnées de G dans toutes base orthonormée de E forment un vecteur gaussien standard.
\bigskip 

\Def 
\sskip 
Pour $n\ge 1$, la distribution $\chi$-carré avec n degrés de liberté, notée $\chi_2(n)$, est la loi de la VA:
$$Z_n=\sum_{i=1}^nG_i^2=||G|| \ \ \text{avec}\ \ G\sim \mc N_n(0,I_n).$$
Son espérance est $n$, sa variance $2n$.
\bigskip 
\subsection{Statistiques d'Echantillons Gaussiens}
On note $S_n^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar X_i)^2$ l'estimateur non biaisé de la variance.
\bigskip 

\Prop 
\sskip 
En considérant $\Prob_{\mu,\sigma^2}$, alors les estimateurs $\bar X_n$ et $S_n^2$ sont indépendants et:
$$\bar X_n \sim \mc N(\mu,\frac{\sigma^2}{n}) \ \ \text{et} \ \ (n-1)\frac{S_n^2}{\sigma^2}\sim \chi_2(n-1).$$
\bskip 
On introduit pour la suite la variable aléatoire réduite $X_i'=\frac{X_i-\mu}{\sigma}\sim\mc N(0,1)$ et on définit comme on s'y attend $\bar{X_n'}$ et $S_n'^2$ telles que:
$$\bar X_n=\mu+\sigma \bar{X_n'} \ \ \text{et} \ \ S_n^2=\sigma^2S_n'^2$$
alors, en notant $E_1=\text{Vect}(\begin{pmatrix}
    1\\
    \vdots\\
    1
\end{pmatrix})$, $E_2=E_1^\bot$ et $e=\frac{1}{\sqrt n}\begin{pmatrix}
    1\\
    \vdots\\
    1
\end{pmatrix}$, il vient: 
$$G_{E_1}=<G,e>e=\bar{X_n'}\begin{pmatrix}
    1\\
    \vdots\\
    1
\end{pmatrix} \ \ \text{et} \ \ ||G_{E_2}||^2=(n-1)(S_n')^2.$$
\bigskip 

\subsection{La Distribution de Student}
Soit $n\ge 1$, alors:
\smallskip 

\Def 
\sskip 
La distribution de Student avec n degrés de liberté, notée $t(n)$, est la loi de la VA: $T_n=Y\sqrt{\frac{n}{Z_n}}$ avec $Z_n\sim \chi_2(n)$ indépendante de $Y\sim \mc N(0,1)$.
\bigskip 

\Prop 
\sskip 
Pour $\Prob_{\mu,\sigma^2}$, alors: $\frac{\bar X_n-\mu}{\sqrt{\frac{S_n^2}{n}}}\sim t(n-1)$.
\bigskip 

\subsection{Régression Linéaire avec Erreurs Gaussiennes}
On s'intéresse ici à $(x_1,y_1),...,(x_n,y_n)\in \R^p\times \R$ et on suppose qu'il existe $\beta\in\R^{p+1}$ et des VAs $\ind \epsilon n$ telles que:
$$y_i = \beta_0 +\beta_1x_i+...+\beta_px_i^p+\epsilon_i.$$
On peut alors réécrire: $Y_n=X_n\beta + \epsilon_n$. L'estimateur de carré minimal (OLS) de $\beta$ est donné par: $\min_\beta ||Y_n-X_n\beta||^2$.
\sskip 
Si on suppose par ailleurs que $\E(\epsilon_n)=0$ et $Cov(\epsilon_n)=\sigma^2I_n$, alors l'OLS est non biaisé et $Cov(\beta)=\sigma^2(X_n^TX_n)^{-1}$ et, si $n>p+1$, alors un estimateur de $\sigma^2$ est donné par:
$$\hat{\sigma}^2=\frac{||Y_n-X_n\hat \beta||^2}{n-p-1}$$
avec $\hat \beta = (X_n^TX_n)^{-1}X_n^TY_n$ qui défini alors l'OLS.
\sskip 
Ensuite, si on suppose que $\ind \epsilon n$ sont des VAs gaussiennes indépendantes centrées de variance $\sigma^2$, alors:
\smallskip 

\Prop 
Les estimateurs $\hat \beta$ et $\hat \sigma^2$ sont indépendants et: 
$$\hat \beta\sim \mc N_{p+1}(\beta, \sigma^2(X_n^TX_n)^{-1}) \ \ \text{et} \ \ (n-p-1)\frac{\hat \sigma^2}{\sigma^2}\sim \chi_2(n-p-1).$$
\bigskip 

\section{Intervalles de Confiance}
\subsection{Définitions Générales}
Soit $\alpha\in (0,1/2)$ la précision désirée pour notre intervalle de confiance.
\bigskip 

\Def \ Intervalle de confiance
\sskip 
Un intervalle de confiance de niveau $1-\alpha$ pour la QI $g(\theta)$ est un intervalle $I_n=[I_n^-,I_n^+]$ tel que $I_n^-$ et $I_n^+$ soient des statistiques et, pour tout $\theta \in \Theta$, $\Prob_\theta (g(\theta)\in I_n)=1-\alpha$.
\bskip 
Il peut être assez difficile, voire impossible, de construire des intervalles de confiance comme définis précédemment (on dit qu'ils sont exacts), on introduit donc les définitions suivantes:
\smallskip 

\Def 
\sskip 
Soit un intervalle $I_n$ tel que $I_n^-$ et $I_n^+$ soient des statistiques, alors cet intervalle est dit:
\begin{itemize}
    \item de confiance asymptotique si, pour tout $\theta\in\Theta$, $\lim_n \Prob_\theta (g(\theta)\in I_n)=1-\alpha$
    \item de confiance par excès si, pour tout $\theta\in\Theta$, $\lim_n \Prob_\theta (g(\theta)\in I_n)\ge 1-\alpha$.
\end{itemize}
\bigskip 
Lorsque $g(\theta)\in \R^d$, on peut alors généraliser la notion d'intervalle de confiance à celle de région de confiance définie, pour un niveau $1-\alpha$, par une fonction $C_n$ de $E^n$ dans l'espace des sous-ensembles mesurables de $\R^d$ telle que:
$$\forall z\in \R^d, \ 1_{z\in C_n(X_n)} \ \ \text{est une statistique et} \ \ \forall\theta\in\Theta, \ \Prob_\theta(g(\theta)\in C_n(X_n)) = 1-\alpha.$$
\subsection{Construction d'Intervalles de Confiance Exact}

\subsubsection{Fonction Pivot}
On commence par une définition générale:
\smallskip 

\Def \ VA libre
\sskip 
Un VA $Q$ est dite libre selon $\Prob_\theta$ si sa loi ne dépend pas de $\theta$.
\bigskip 

\Def \ Fonction pivot
\sskip 
Un fonction pivot pour $g(\theta)$ est une fonction $\pi_n:E^n\times g(\Theta)\to \R$ telle que $\pi_n(X_n,g(\theta))$ est libre.
\bigskip 

\subsubsection{Exemple dans le Modèle Gaussien}
On s'intéresse à la moyenne $\mu$ dans un modèle Gaussien qu'on estime classiquement avec $\bar X_n$. La loi de $\bar X_n$ selon $\Prob_{\mu,\sigma^2}$ est $\mc N(\mu,\sigma^2/n)$. Il vient alors:
$$Y_n=\frac{\bar X_n -\mu}{\sqrt{\sigma^2/n}}\sim \mc N(0,1) \ \ \text{est libre.}$$
Néanmoins, la fonction:
$$\pi_n(x_n,\mu)=\frac{\bar x_n -\mu}{\sqrt{\sigma^2/n}}$$
n'est pas une fonction pivot puisqu'elle dépend de $(\mu,\sigma^2)$ à travers $\mu$ et $\sigma$ et non uniquement de $\mu$.
\sskip 
Supposons donc momentanément que $\sigma^2$ soit connu, alors $\pi_n$ devient une fonction pivot. Il vient alors, pour tout réels a et b tels que $a<b$:
$$\Prob_{\mu,\sigma^2}(Y_n\in[a,b])=\frac{1}{\sqrt{2\pi}}\int_a^b\exp(-x^2/2)dx$$
donc, pour tout choix de a et b tels que:
$$\frac{1}{\sqrt{2\pi}}\int_a^b\exp(-x^2/2)dx=1-\alpha$$
alors l'intervalle $[\bar X_n - b\sqrt{\sigma^2/n},\bar X_n - a\sqrt{\sigma^2/n}]$ est un intervalle de confiance exact de précision $1-\alpha$ pour $\mu$.
\sskip
Rappelons par ailleurs que l'on définit par $\phi_r$ le quantile d'ordre $r$ de la distribution gaussienne standard, alors a et b permettent de satisfaire l'égalité attendue si et seulement si:
$$\exists r\in [0,\alpha]:\ a=\phi_r, \ b=\phi_{r+1-\alpha}.$$
Pour une telle paire $(a,b)$ et l'intervalle de confiance exact $[\bar X_n - b\sqrt{\sigma^2/n},\bar X_n - a\sqrt{\sigma^2/n}]$, la probabilité de sous-estimer $\mu$ est $r$, celle de la sur-estimer est $\alpha-r$.
\bskip 
Supposons désormais que $\sigma^2$ ne soit pas connu. Une idée classique consiste alors à remplace $\sigma^2$ par l'estimateur non-biaisé de la variance $S_n^2$ et on considère:
$$Y_n'=\frac{\bar X_n -\mu}{\sqrt{S_n^2/n}}\sim t(n-1) \ \ \text{est libre.}$$
Il vient alors que:
$$\pi_n(x_n,\mu)=\frac{\bar x_n -\mu}{\sqrt{s_n^2/n}} \ \ \text{avec} \ \ s_n^2=\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar x_i)^2$$
est une fonction pivot.
\sskip 
En conséquence, pour tout réels a et b tels que $a<b$, il vient:
$$\Prob_{\mu,\sigma^2}(Y_n'\in[a,b])=\int_a^bp_{n-1}(x)dx \ \ \text{avec} \ \ p_{n-1} \ \ \text{la densité de la loi} \ \ t(n-1).$$
Une nouvelle fois, dès que le couple $(a,b)$ vérifie:
$$\int_a^bp_{n-1}(x)dx=1-\alpha$$
on obtient un intervalle de confiance de précision 1-$\alpha$ pour $\mu$.
\bigskip 

\subsubsection{Résumé de la Méthode}
On commence par trouver une fonction pivot $Q_n=\pi_n(x_n, g(\theta))$, on réécrit la condition $Q_n\in[a,b]$ comme $g(\theta)\in I_n$ où les extrémités de $I_n$ sont des statistiques. Enfin on choisit un couple $(a,b)$ qui satisfait: $\Prob(Q_n\in [a,b])=1-\alpha$, ce qui revient à choisir $a=q_{n,r}$ et $b=q_{n,r+1-\alpha}$ avec $0\le r \le \alpha$ et $q_{n,r}$ le quantile d'ordre $r$ de $Q_n$.
\bigskip 

\subsection{Construction d'Intervalle de Confiance Asymptotique}
On rappelle que $\phi_r$ définit le quantile d'ordre $r$ de la distribution gaussienne standard.
\bigskip 

\Prop \ Intervalle de confiance asymptotique
\sskip 
Soit $Z_n$ un estimateur consistant et convergent normalement de $g(\theta)$, on note $V(\theta)$ sa variance asymptotique. On suppose qu'un estimateur consistant $\hat V_n$ de $V(\theta)$ est connu, alors:
$$\forall \alpha \in (0,1/2), \ I_n=[Z_n-\phi_{1-\alpha/2}\sqrt{\frac{\hat V_n}{n}},Z_n+\phi_{1-\alpha/2}\sqrt{\frac{\hat V_n}{n}}]$$
est un intervalle de confiance asymptotique avec une précision de $1-\alpha$ pour $g(\theta)$.
\bskip 
En général il n'est pas difficile de trouver un estimateur consistant de $V(\theta)$, dès que $V$ est continue et que $\hat \theta_n$ est un estimateur consistant de $\theta$, alors on a simplement: $\hat V_n=V(\hat \theta_n)$.
\bigskip 

\subsection{Construction d'Intervalle de Confiance par Excès}
On s'intéresse au cas où on ne possède pas de fonction pivot, comme dans le cas d'une loi de Bernoulli. On va alors construire des intervalles de confiance par excès à l'aide des inégalités de concentration.
\bigskip 

\Def \ Inégalité de concentration
\sskip 
Une inégalité de concentration pour une variable aléatoire $Y$ est une inégalité de la forme: 
$\Prob(|Y-\E(Y)|\ge r)\le c_Y(r)$ pour une fonction de concentration $c_Y$ convergent asymptotiquement vers 0.
\bskip 
Si $Z_n$ est un estimateur non-biaisé de $g(\theta)$ et qui vérifie une inégalité de concentration telle que:
$$\forall r>0, \ \sup_\theta \Prob_\theta (|Z_n-g(\theta)|\ge r)\le c_{Z_n}(r)$$
alors tout $r_{n,\alpha}>0$ tel que $c_{Z_n}(r_{n,\alpha})\le\alpha$ produit l'intervalle de confinace par excès: $[Z_n-r_{n,\alpha}, Z_n +r_{n,\alpha}]$ pour $g(\theta)$.
\bigskip 
\subsubsection{L'Inégalité de Bienaymé-Chebychev}
C'est l'inégalité classique:
$$\Prob( |Y-\E(Y)|\ge a)\le \frac{V(Y)}{a^2}.$$
Pour un modèle de Benoulli dans lequel on utilise $\bar X_n$ comme estimateur de $p$, alors, pour tout $a>0$, il vient:
$$\Prob_p(|\bar X_n - p|\ge a)\le \frac{p(1-p)}{a^2}$$
donc, pour $a$ tel que: $\frac{p(1-p)}{a^2}\le \alpha$, il vient:
$$\Prob_p(p\in[\bar X_n - a/\sqrt{n},\bar X_n + a/\sqrt{n} ])\le 1-\alpha.$$
Il reste à trouver une telle valeur de $a$ qui ne dépende pas de p, pour les VAs bornées, on peut utiliser le lemme suivant:
\smallskip 

\underline{Lemme:} Borne universelle de la variance
\sskip 
Soit Y une VA à valeurs dans $[0,1]$, alors: $V(Y)\le \frac{1}{4}$.
\bskip 
Il suffit alors de prendre $a=\frac{1}{2\sqrt{\alpha}}$.
\bigskip 
\subsubsection{Inégalité de Hoeffding}
On commence par introduire le lemme suivant:
\smallskip 

\underline{Lemme:} Inégalité de Hoeffding
\sskip 
Soient $\ind X n$ des VAs iid à valeurs dans $[0,1]$, alors, pour tout $n\ge 1$ et $r>0$, il vient:
$$\Prob(\sum_{i=1}^n(X_i-\E(X_i))\ge r\sqrt{n})\le \exp(-2r^2).$$
\sskip 
Il vient alors:
$$\Prob(|\bar X_n-\E(X_1)|\ge r/\sqrt{n})\le 2\exp(-2r^2).$$
\bigskip

\section{Estimateur du Maximum de Vraisemblance}
Jusqu'à présent, on a vu des méthodes d'estimation, de calcul d'intervalle de confiance qui reposent sur le choix de fonctions particulières. A l'inverse l'estimateur du maximum de vraisemblance permet d'estimer les paramètres d'un modèle de façon plus générale.
\subsection{Vraisemblance d'un Echantillon et Estimateur}
On s'intéresse à $(\ind X n)$ des VAs iid dans $\R^n$.
\bigskip 

\Def \ Vraisemblance d'une observation
\sskip 
Soit $x^*_n=(\ind x n)$ une valeur possible de $X^*_n=(\ind X n)$, la vraisemblance de cette observation est donnée par la fonction: 
$$L_n(x^*_n, \cdot) = \theta \mapsto \prod_{i=1}^np(x_i,\theta) \ \ \text{avec} \ \ p(x_i,\theta)= \Prob_\theta(X_i=x_i).$$
\bskip 
Dans le cas discret, alors: $L_n(x_n^*,\theta)= \Prob_\theta(X_1=x_1,...,X_n=x_n)$, si $\Prob_\theta$ admet une densité selon la mesure de Lebesgue, alors elle est donnée par: $x_n^*\mapsto L_n(x_n^*,\theta)$.
\bskip 
L'estimation du maximum de vraisemblance consiste à chercher le paramètre $\theta^*$ qui rend les valeurs observées les plus probables.
\smallskip 

\Def \ Estimateur du maximum de vraisemblance 
\sskip 
Supposons que, pour tout $x_n^*$, $\theta\mapsto L_n(x_n^*, \theta)$ atteigne un maximum global en $\theta^* = \theta_n(x_n^*)$. L'estimateur du maximum de vraisemblance (MLE) de $\theta$ est la statistique:
$$\hat{\theta_n} =\theta_n(X_n^*).$$
Dans le cas où il y a plusieur maximum, on peut simplement prendre $\hat{\theta_n}\in\arg\max_\theta L_n(x_n^*,\theta)$.
\bskip 
Si la fonction de vraisemblance est différentiable, on peut alors calculer $\hat{\theta_n}$ en cherchant les zéros du gradient. Dans cette perspective, il peut être intéressant de considérer la dérivée du logarithme de la vraisemblance:
$$l_n(x_n^*,\theta) = \log (L_n(x_n^*,\theta)).$$
\bigskip 

\subsection{Exemples}
\subsubsection{Modèle de Bernoulli}
On a alors: 
$$L_n(x_n^*, p )=\prod_{i=1}^n \Prob_p(X_i=x_i) = \prod_{i=1}^n p^{x_i}(1-p)^{1-x_i}$$
donc: $$l_n(x_n^*,p)=\sum_{i=1}^n(x_i\log(p)+(1-x_i)\log(1-p))=n\bar X_n\log(p) + n(1-\bar X_n)\log(1-p).$$
On peut alors calculer: 
$$\frac{\partial l}{\partial p}(x_n^*,p^*) = 0 \Longleftrightarrow (1-p^*)\bar X_n - p^* (1-\bar X_n) = 0 \Longleftrightarrow p^* =\bar X_n.$$
Il reste alors à s'assurer que $p^*$ correspond bien à un maximum, dans le cas du modèle de Bernoulli c'est bien le cas.
\bigskip 

\subsubsection{Cas du Modèle Gaussien}
On a alors:
$$L_n(x_n^*,\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}^n}\prod_{i=1}^n\exp(\frac{-(x_i-\mu)^2}{2\sigma^2})$$
donc: 
$$\frac{\partial l_n}{\partial \mu}(x_n^*,\mu^*,{\sigma^2}^*) = \frac{1}{{\sigma^2}^2}\sum_{i=1}^n(x_i - \mu^*) \ \ \text{et}\ \ \frac{\partial l_n}{\partial \sigma^2}(x_n^*,\mu^*,{\sigma^2}^*) = -\frac{n}{2\pi\sigma^*}+\sum_{i=1}^n\frac{(x_i-\mu^*)^2}{{\sigma^3}^*}.$$
On trouve alors : $\mu^*=\bar X_n$ et ${\sigma^2}^*=S_n^2$ qui sont les estimateurs qu'on trouve également avec la méthode des moments.
\bigskip 

\subsection{Optimalité du MLE}
Lorsqu'on a plusieurs estimateurs du même paramètre on peut comparer leur MSE, voire leur variance asymptotique s'ils sont asymptotiquements normaux. Le MLE est asymptotiquement normal, sa variance asymptotique est par ailleurs optimale dans les modèles réguliers.
\subsubsection{Modèle Régulier et Information de Fisher}
On rappelle que $\nabla\phi_\theta = (\frac{\partial \phi}{\partial\theta_1},...,\frac{\partial \phi}{\partial\theta_d})$.
\bigskip 

\Def \ Modèle régulier 
\sskip 
Un modèle paramétrique est régulier si: $\Theta$ est ouvert et:
\begin{itemize}
\item $\forall x_1,\forall \theta\in\Theta, \ L_1(x_1,\theta)>0$
\item $\forall x_1, \ \theta\mapsto l_1(x_1,\theta)\in \mc C^1(\Theta)$
\item $\forall \theta\in \Theta, \ \E_\theta(||\nabla_\theta l_1(X_1,\theta)||^2)<\pinf.$
\end{itemize}
\bigskip 

\Def \ Score dans un modèle régulier 
\sskip 
Dans un modèle régulier, on appelle score le vecteur aléatoire $\nabla_\theta l_1(X_1,\theta) = (\frac{1}{p(X_1,\theta)}\frac{\partial p}{\partial\theta_i}(X_1,\theta))_i$.
\sskip 
L'information de Fisher $I(\theta)$ d'un modèle régulier est: 
$$I(\theta) = Cov_\theta(\nabla_\theta l_1(X_1,\theta)).$$

\Prop 
\sskip 
On a, pour tout $\theta$, $\E_\theta(\nabla_\theta l_1(X_1,\theta)) = 0$ et les coefficients de $I(\theta)$ sont:
$$I_{i,j}(\theta) = \E_\theta(\frac{\partial l_1}{\partial \theta_i}(X_1,\theta)\frac{\partial l_1}{\partial \theta_j}(X_1,\theta)) = -\E_\theta(\frac{\partial^2l_1}{\partial\theta_i\partial\theta_j}(X_1,\theta)).$$
\bigskip 

\subsubsection{Estimateur Efficace}
On peut utiliser l'information de Fisher pour définir une notion d'efficacité pour les modèles non-biaisés.
\smallskip 

\Theo \ Borne de Cramér-Rao 
\sskip 
Pour un modèle régulier tel que $I(\theta)>0$ pour tout $\theta$, on note $\tilde \theta_n = t_n(X_n)$ un estimateur non-biaisé de $\theta$ avec la matrice de covariance $K_n(\theta)= Cov_\theta(\tilde \theta_n)$; alors:
$$K_n(\theta) \succeq \frac{I^{-1}(\theta)}{n}.$$
\bigskip 

\Def \ Estimateur efficace 
\sskip 
Un estimateur est dit efficace s'il est non-biaisé et si sa matrice de covariance vérifie $K_n(\theta) = \frac{I^{-1}(\theta)}{n}$.
\bskip 
On rappelle que si $\tilde \theta_n$ est un estimateur non-biaisé, alors: $MSE(\tilde \theta_n,\theta)=\V_\theta(\tilde\theta_n)=tr(K_n(\theta))\ge \frac{tr(I^{-1}(\theta))}{n}$. En conséquences, dans un modèle régulier, les estimateurs efficaces minimisent la MSE pour les estimateurs non-biaisés.
\bskip 
\section{Estimation Bayesienne}
Les techniques utilisées jusqu'à présent sont essentiellement motivées par le fait que les estimateurs sont consistants. Ainsi, si suffisament de données sont relevées, on peut retrouver la valeur réelle de la quantité d'intérêt, c'est l'approche fréquentiste.
\sskip 
Néanmoins, dans le certains cas, comme lorsque les données sont manquantes, il peut être intéressant d'avoir des connaissances, indépendantes des données observées, a priori sur la valeur de la quantité d'intérêt. C'est le principe des l'inférence bayésienne.
\bskip 
\subsection{Formalisation de l'Estimation Bayésienne}
On travaille toujours dans le cadre d'un modèle paramétrique $\mc P = \{\Prob_\theta; \ \theta\in\Theta\}$ où $\Theta$ est fini ou un sous-ensemble de $\R^q$.
\bskip 
La distribution a priori est une mesure de probabilité sur $\Theta$ qui évalue la crédibilité de chaque valeur de $\theta$ avant d'observer les données.
\bigskip 

\Def \ Distribution a posteriori 
\sskip 
Pour une distribution a priori $Q$, la distribution a posteriori est la mesure de probabilité $Q(\cdot | x_n^*)$ définie sur $\Theta$ par:
$$Q(d\theta | x_n^*) = \frac{L_n(x_n^*,\theta)Q(d\theta)}{\int_{\omega\in\Theta} L_n(x_n^*,\omega)Q(d\omega)},$$
où $L_n(x_n^*,\theta)$ est la vraisemblance de l'observation $x_n^*$.
\sskip 
Ainsi, pour tout sous-ensemble $B\subset \Theta$ mesurable, il vient:
$$Q(B | x_n^*) = \frac{\int_{\theta\in B}L_n(x_n^*,\theta)Q(d\theta)}{\int_{\omega\in\Theta} L_n(x_n^*,\omega)Q(d\omega)}.$$
\bskip 
Plus généralement, si $\Theta$ est dénombrable, alors:
$$Q(d\theta | x_n^*) = \frac{L_n(x_n^*,\theta)Q(\theta)}{\sum_{\omega\in\Theta} L_n(x_n^*,\omega)Q(\omega)},$$
si $Q$ a une densité $q$, alors $Q(\cdot, x_n^*)$ a la densité:
$$q(\theta| x_n^*)=\frac{L_n(x_n^*,\theta)q(\theta)}{\int_{\omega\in\Theta}L_n(x_n^*,\omega)q(\omega)d\omega}.$$
\bigskip 
La distributtion a posteriori doit être interprétée comme la crédibilité de $\theta$ étant donnée l'observation de $x_n^*$.
\bskip 
\subsection{Estimateur Bayesien}
La distribution a posteriori possède toutes les informations nécessaires pour l'étude de $\theta$. Toutefois, en tant que mesure de distribution de probabilité, elle n'est pas évidente à manipuler, il est donc intéressant d'introduire les notions d'estimateur (Bayesien) ou d'intervalle de confiance.
\bigskip 

\Def \ Moyenne postérieure (Sous l'hypothèse $\Theta\subset\R^q$ convexe)
\sskip 
La moyenne postérieure (PM) est la moyenne de la distribution a posteriori:
$$\widehat{\theta_n^{PM}} = \theta_n^{PM}(X_n), \ \ \text{avec} \ \ \theta_n^{PM}(x_n^*)= \int_{\theta\in\Theta}\theta Q(d\theta|x_n^*).$$
La VA $\widehat{\theta_n^{PM}}$ est une statistique et donc un estimateur de $\theta$ dans le sens fréquentiste.
\bskip 
Dans la définition suivante, on suppose que ou bien $\Theta$ est discret et on pose $q(\theta)=Q(\{\theta\})$ ou bien $Q$ a une densité $q$.
\smallskip 

\Def \ Maximum a posteriori (MAP)
\sskip 
Supposons que, pour tout $x_n^*\in E^n$, la fonction $\theta\mapsto q(\theta|x_n^*)$ possède un maximum unique atteint en $\theta = \theta_n^{MAP}(x_n^*)$, alors le MAP est défini par :
$\widehat{\theta_n^{MAP}} = \theta_n^{MAP}(X_n)$. Comme la PM, le MAP est un estimateur de $\theta$ dans le sens fréquentiel.
\bskip 
Il est généralement attendu que la distribution a posteriori se concentre autour de $\theta$ de façon asymptotique. Quand $\Theta$ est discrèt, la définition de ce phénomène est immédiate:
\smallskip 

\Def \ Consistence de la distribution a posteriori (cas discret)
\sskip 
L'estimateur Bayesien de $\theta$ avec la distribution a priori $Q$ est consistant si:
$$\forall \theta \in\Theta, \ \lim_{n\to\infty}Q(\theta|X_n)=1, \ \ \text{selon } \Prob_\theta.$$
Pour le cas général, la définition est un peu plus technique:

\Def \ Consistence de la distribution a posteriori (cas $\Theta\subset\R^q$)
\sskip 
L'estimateur Bayesien de $\theta$ avec la distribution a priori $Q$ est consistant si:
$$\forall \theta \in\Theta, \ \forall\epsilon >0, \ \lim_{n\to\infty}Q(\{\omega\in\Theta | \ ||\omega - \theta||\ge\epsilon\}|X_n)=0, \ \ \text{presque sûrement selon } \Prob_\theta.$$

\Prop \ Condition suffisante de consistance
\sskip 
Pour tout $x_n^*\in E^n$, on définit la variance de la distribution a posteriori par:
$$V_n(x_n^*)=\int_{\theta\in\Theta}||\theta - \theta_n^{PM}(x_n^*)||^2Q(d\theta|x_n^*).$$
On peut alors vérifier que, si la PM $\widehat{\theta_n^{PM}}$ est fortement consistante et si $V_n(X_n)\longrightarrow 0$ presque sûrement selon $\Prob_\theta,$ pour tout $\theta$, alors l'estimateur bayésien de $\theta$, pour la distribution a priori $Q$, est consistant.
Dans le cas où $\Theta\subset\R^q$, on peut alors introduire le concept d'intervalle de crédibilité de niveau $1-\alpha$ qui définit un intervalle $I$ dont les bornes sont des statistiques telles que:
$$Q(I|X_n)=1-\alpha.$$
Ces intervalles sont l'équivalent Bayesien des intervalles de confiances.
\bskip 
\section{Formalisme des Tests d'Hypothèses Statistiques }
\subsection{Formalisme Général}
On se donne ici $\Theta\subset\R^q$ et $H_0,H_1$ une partition de $\Theta$. $H_0$ est l'hypothèse nulle qui correspond à une absence d'effet. $H_1$ est l'hypothèse alternative, elle correspond à la présence d'un effet et on cherchera en général à prouver que $\theta\in H_1$.
\bigskip

\Def \ Test 
\sskip 
Un test de $H_0$ contre $H_1$ est une règle de décision déterminant, pour une observation, si $\theta\in H_0$ ou $\theta\in H_1$. Il s'agit d'une fonction déterministe de $E^n$ dans $\{H_0,H_1\}$.
\bigskip 

\Def \ Région de rejet 
\sskip
Il s'agit de l'ensemble de test $W_n$ pour lesquels l'hypothèse $H_0$ est rejetée.
\bskip 
Dans le cadre d'un modèle de Bernoulli où l'on chercherait à savoir si une pièce est truquée ou non, alors on aurait:
$$H_0 = \{1/2\} \ \ \text{et} \ \ H_1=[0,1]\backslash \{1/2\}.$$
\subsubsection{Erreurs de Type I et II}
Néanmoins, comme le test $x_n^*$ n'est que la réalisation d'une VA $X_n$, il se peut qu'il donne un résultat incorrect. On distingue alors deux types d'erreurs.
\smallskip 

\Def \ Erreurs de type I et II 
\sskip 
Une erreur de type I est une rejection incorrecte de l'hypothèse nulle, elle est mesurée par le risque de type I: $$\theta\in H_0\mapsto \Prob_\theta (X_n\in W_n).$$
Une erreur de type II est la validation incorrecte de l'hypothèse nulle, elle est mesurée par le risque de type II:
$$\theta\in H_1\mapsto \Prob_{\theta}(X_n\notin W_n).$$

\Def Puissance statistique d'un test 
\sskip 
La puissance statistique d'un test est définie par la fonction: 
$$\theta \in H_1 \mapsto \Prob_\theta (X_n\in W_n) = \Prob_\theta(W_n) = 1 - \text{erreur de type II}.$$
\Def \ Propriétés asymptotiques
\sskip 
Un test est dit consistant si: $\forall\theta\in H_1, \ \lim_n \Prob_\theta(X_n\in W_n) = 1$. Le test est de niveau asymptotique $\alpha $ si:
$$\alpha = \sup_{\theta\in H_0}\limsup_{n}\Prob_\theta (X_n\in W_n).$$
Une question naturelle se pose alors: comment choisir un test pertinent ? D'après les définitions précédentes, il est tout d'abord notable que:
\begin{itemize}
    \item seuls les test de même niveau $\alpha$ peuvent être comparés
    \item un test est alors préférable à un autre s'il est plus puissant, c'est-à-dire que son erreur de type II est plus faible.
\end{itemize}
\subsubsection{Procédure Générale Pour Construire un Test}
On commence par définir les ensembles $H_0$ et $H_1$ (il est souvent judicieux de construire $H_1$ en premier).
\sskip 
On définit alors la région de rejet $W_n$, on préfère les cas où on peut écrire, avec $g:\Theta\to\R$ et $g_0\in\R$:
\begin{itemize}
    \item $H_1=\{g(\theta)>g_0\} \ \ \text{ou} \ \ H_1=\{g(\theta)<g_0\}$ on dit alors que le test est one-sided
    \item $H_1=\{g(\theta)\ne g_0\}$ on dit alors que le test est two-sided.
\end{itemize}
Supposons par ailleurs qu'on possède un estimateur $Z_n$ de $g(\theta)$, on peut alors réécrire $W_n$ comme:
\begin{itemize}
    \item $\{Z_n\ge g_0+a_n\}$ si $H_1=\{g(\theta)>g_0\}$
    \item $\{Z_n\le g_0-a_n\}$ si $H_1=\{g(\theta)<g_0\}$
    \item $\{Z_n\notin (g_0-a_n, g_0+b_n)\}$ sinon
\end{itemize}
pour $a_n,b_n\ge 0$.

On se place pour l'instant dans le cadre d'un test one-sided de la forme $H_1=\{g(\theta)>g_0\}$. Pour tout $a_n\ge 0$, l'erreur de type I est de la forme:
$$\Prob_\theta (W_n) = \Prob_\theta (Z_n\ge g_0 + a_n), \ \ \theta\in H_0.$$
Supposons par ailleurs qu'il existe $\theta_0\in H_0$ tel que:
$$\forall a_n\ge 0, \ \ \sup_{\theta\in H_0}\Prob_\theta (Z_n\ge g_0 + a_n)=\Prob_{\theta_0}(Z_n\ge g_0 + a_n).$$
Alors, en notant $z_{\theta_0,n,r}$ le quantile d'ordre $r$ de la loi de $Z_n$ selon $\Prob_{\theta_0}$, on obtient:
$$\sup_{\theta\in H_0}\Prob_\theta (W_n)<\alpha \ \ \text{si et seulement si } g_0+a_n\ge z_{\theta_0,n,1-\alpha}.$$
De plus, pour tout $\theta\in H_1$, le risque de type II $P_\theta(Z_n<g_0 + a_n)$ est une fonction croissante de $g_0+a_n$ qui est donc minimum en $z_{\theta_0,n,1-\alpha}$. La région de rejet finale de puissance statistique maximum sous la contrainte d'un niveau inférieur à $\alpha$ est:
$$W_n = \{Z_n\ge z_{\theta_0,n,1-\alpha}\}.$$
Si $H_1 = \{g(\theta)<g_0\}$, alors on peut écrire symétriquement: $W_n=\{Z_n\le z_{\theta_0,n,\alpha}\}$ comme région de rejet finale.
\bigskip

Considérons désormais le cas d'un test two-sided. On suppose de nouveau qu'il existe $\theta_0\in H_0$ tel que:
$$\forall a_n,b_n, \ \sup_{\theta\in H_0}\Prob_\theta(Z_n\notin (g_0-a_n,g_0+b_n))=\Prob_{\theta_0}(Z_n\notin (g_0-a_n, g_0+b_n)).$$
Il est alors clair que le risque de type I est borné supérieurement par $\alpha$ dès qu'on choisit $a_n,b_n$ tels que:
$$\exists r\in[0,\alpha]: \ g_0-a_n\le z_{\theta_0,n,r} \ \ \text{et} \ \ g_0+b_n\ge z_{\theta_0,n,1-\alpha+r}.$$
De plus le risque de type II est nécessairement réduit si ces inégalités sont transformées en égalités, mais la valeur de $r$ qui minimise le risque de type II risque, en général, de dépendre de $\theta$, ce qui n'était pas le cas pour l'étude one-sided. Il est alors courant de choisir $r=\alpha/2$ de sorte que:
$$W_n = \{Z_n\le z_{\theta_0,n,\alpha/2} \ \ \text{ou} \ \ Z_n\ge z_{\theta_0,n,1-\alpha/2}\}.$$
Il arrive parfois que la loi de $Z_n-g_0$ soit symétrique pour $\Prob_{\theta_0}$, donc:
$$z_{\theta_0,n,\alpha/2} = g_0 - z_{n,1-\alpha/2}, \ \ z_{\theta_0,n,1-\alpha/2} = g_0 + z_{n,1-\alpha/2}$$
avec $z_{n,1-\alpha/2}$ le quantile d'ordre $1- \alpha/2$ de la loi de $Z_n-g_0$, on peut alors réécrire:
$$W_n = \{|Z_n-g_0|\ge z_{n,1-\alpha/2}\}.$$

Pour les deux cas, la consistance se déduit de la consistance de $Z_n$.
\smallskip 

\Prop \ Consistance
\sskip 
Soit $W_n$ une région de rejet dont la forme est l'une de celle décrite précédemment, alors si $Z_n$ est un estimateur consistant de $g(\theta)$, alors le test est consistant.
\bskip 
\subsubsection{p-Value}
On traite tout d'abord le cas particulier dans lequel il existe une statistique $\zeta_n (X_n)$ et $\theta_0\in H_0$ tel que $W_n = \{\zeta(X_n)\ge \zeta_{\theta_0,n,1-\alpha}\}$ où $\zeta_{\theta_0,n,1-\alpha}$ est le quantile d'ordre $1-\alpha$ de $\zeta_n(X_n)$ pour $\Prob_{\theta_0}$.
\sskip 
Ces hypothèses couvrent les cas suivants:
\begin{itemize}
    \item les test one-sided de la rubrique précédente avec $\zeta_n(X_n)=Z_n$ si $H_1=\{g(\theta)>g_0\}$, $\zeta_n (X_n) = -Z_n$ dans l'autre cas
    \item les test two-sided de la rubrique précédente dans le cas spécifique de symétrie avec $\zeta(X_n)=|Z_n - g_0|$. 
\end{itemize}

\Def \ p-value
\sskip 
Dans les cas précis cités précédemment, pour tout $x_n^*\in E^n$ la p-value de l'observation $x_n^*$ est:
$$p-value = \Prob_{\theta_0}(\zeta(X_n)\ge \zeta(x_n^*)).$$
La p-value doit être comprise comme la probabilité selon $H_0$ que la statistique du test prenne des valeurs plus défavorables à la validation de $H_0$ que les valeurs observées dans les données.
\bigskip 

\Prop \ p-value et niveau 
\sskip 
Sous les hypothèses précédentes, il vient que $ H_0$ est rejetée si et seulement si $p-value\le\alpha$.
\bigskip 
\subsubsection{Dualité entre Tests et Régions de Confiance}
Considérons des hypothèses nulles et alternatives de la forme: 
$$H_0 = \{g(\theta) = g_0\}, \ \ H_1=\{g(\theta)\neq g_0\}$$
pour $g:\Theta\to \R^d, \ g_0\in g(\Theta)$. Supposons de plus qu'une région de confiance $C_n$ de niveau $1-\alpha$ soit disponible pour $g(\theta)$, alors, par définition :
$$\forall\theta\in H_0, \ \Prob_\theta (g_0\notin C_n)=\Prob_\theta(g(\theta)\notin C_n)=\alpha,$$
de sorte que le test avec la région de rejet $W_n=\{g_0\notin C_n\}$ soit de niveau $\alpha$.
\bskip 
\subsection{Comparaisons Multiples}
Supposons qu'on est construit un test avec la région de rejet $W_n$ et le niveau $\alpha$ pour des hypothèses $H_0$ et $H_1$. On suppose de plus que $H_0=\{\theta_0\}$, si on observe $X_n$ distribuée selon $H_0$, alors la probabilité d'un faux positif est, par construction, $\alpha$. Ainsi, pour $m$ observations indépendantes, la probabilité d'avoir au moins un faux positifs devient:
$$\Prob_{\theta_0}(\cup_{k=1}^m\{X_{k,n}\in W_n\})=1-\prod_{k=1}^m\Prob_{\theta_0}(X_{k,n}\notin W_n)=1-(1-\alpha)^m\longrightarrow_{m\to\pinf}1.$$
Ainsi, en répétant suiffisament de fois l'expérience, alors on peut conclure que l'effet qu'on cherche est présent alors même que dans les faits il ne l'est pas.
\bigskip 
\section{Test dans le Modèle Gaussien}
Dans ce chapitre on travail sur des échantillons $\mathbb{X}_n =(\ind X n)$ qui sont iid selon $\mc N(\mu,\sigma^2)$.
\subsection{Test Two-sided pour la Moyenne}
On fixe tout d'abord $\mu_0\in\R$ et on considère les hypothèses $H_0=\{\mu=\mu_0\}$ et $H_1=\{\mu\ne\mu_0\}$, on suppose tout d'abord que $\sigma^2$ est connu.
\sskip 
En suivant la procédure présentée dans le chapitre précédent, on peut construire: 
$$W_n =\{|\bar X_n - \mu_0|\ge \sqrt{\frac{\sigma^2}{n}}\phi_{1-\alpha/2}\}$$
qui est une région de rejet consistante de niveau $\alpha$, on la réécrit sous la forme:
$$W_n =\{|Z_n|\ge \phi_{1-\alpha/2}\} \ \ \text{avec } Z_n=\frac{\bar X_n -\mu_0}{\sqrt{\sigma^2/n}}$$
on dit que $Z_n$ est le Z-score, selon $H_0$, cette statistique est distribuée selon $\mc N(0,1)$ et est libre.
\bskip 
Si $\sigma^2$ est inconnu, alors on peut utiliser $T_n=\frac{\bar X_n-\mu_0}{\sqrt{S_n^2/n}}$ qui suit la loi $t(n-1)$ selon $H_0$, on l'appelle le t-score.
\bskip 
\subsection{Test One-sided pour la moyenne}
On s'intéresse désormais à $H_0=\{\mu\le\mu_0\}$ et $H_1 = \{\mu>\mu_0\}$.
\sskip 
Si $\sigma^2$ est connue, alors on a:
$$W_n=\{Z_n\ge \phi_{1-\alpha}\}$$
qui est une région de rejet consistante de niveau $\alpha$.
\bskip 
Si $\sigma^2$ est inconnu, alors on a:
$$W_n=\{T_n\ge t_{n-1,1-\alpha}\}$$
qui vérifie les mêmes propriétés.
\bigskip 
\subsection{Test sur Deux Echantillons}
On s'intéresse ici au problème de l'homogénéité de deux populations, on observe donc deux échantillons $\mathbb{X}_{1,n}$ et $\mathbb{X}_{2,n}$ distribués selon $\Prob_1$ et $\Prob_2$, on cherche donc à savoir si $\Prob_1=\Prob_2$.
\bskip 
On étudie pour ce faire les tests de Fisher avec les hypothèses: $H_0=\{\sigma_1^2=\sigma_2^2\}$ et $H_1=\{\sigma_1^2\ne\sigma_2^2\}$, et de Student pour lequel on suppose que $\sigma_1^2=\sigma_2^2$ avec les hypothèses:
$$H_0=\{\mu_1=\mu_2\},\ H_1=\{\mu_1\ne\mu_2\} \ \ \text{ou} \ \ H_0=\{\mu_1\le\mu_2\}, \ H_0=\{\mu_1>\mu_2\}.$$
\subsubsection{Test de Student}
On suppose que la variance des deux échantillons est identique, on ne suppose néanmoins pas la connaître.
\sskip 
On considère d'abord les hypothèses $H_0=\{\mu_1=\mu_2\}$ et $H_1=\{\mu_1\ne\mu_2\}$, alors, selon $H_0$, on a: 
$$\bar X_{1,n_1} - \bar X_{2,n_2}\sim \mc N(0,\sigma^2/n_1 + \sigma^2/n_2)$$
et 
$$\frac{(n_1-1)S^2_{1,n_1} + (n_2-1)S^2_{2,n_2}}{\sigma^2}\sim \chi_2(n_1+n_2-2)$$
et ces deux VAs sont indépendantes. En conséquence:
$$T_{n_1,n_2} = \frac{\frac{\bar X_{1,n_1} - \bar X_{2,n_2}}{\sqrt{\sigma^2(1/n_1+1/n_2)}}}{\frac{(n_1-1)S^2_{1,n_1} + (n_2-1)S^2_{2,n_2}}{\sigma^2(n_1+n_2-2)}}\sim t(n_1+n_2-2).$$
On peut donc s'intéresser à la réjion de rejet $W_{n_1,n_2} = \{|T_{n_1,n_2}|\ge t_{n_1+n_2-2,1-\alpha/2}\}$ qui est de niveau $\alpha$ et est constistante lorsque $n_1$ et $n_2$ tendent vers l'infini.
\bskip 
Si les deux hypothèses sont plutôt $H_0=\{\mu_1\le\mu_2\}$ et $H_1=\{\mu_1>\mu_2\}$, alors, par des arguments similaires, on obtient la région de rejet $W_{n_1,n_2}=\{T_{n_1,n_2}\ge t_{n_1+n_2-2,1-\alpha}\}$ qui est également de niveau $\alpha$ et consistante lorsque $n_1$ et $n_2$ tendent vers l'infini.
\bigskip 
\subsubsection{Test de Fisher}
On considère désormais les hypothèses $H_0=\{\sigma_1^2=\sigma_2^2\}$ et $H_2=\{\sigma_1^2\ne\sigma_2^2\}$. Comme $\sigma_1^2$ et $\sigma^2_2$ sont estimés par $S_{n_1}^2$ et $S_{n_2}^2$, alors on doit rejeter $H_0$ lorsque ces deux quantités sont éloignées. Afin donc de construire une statistique qui mesure cet éloignement et qui soit libre selon $H_0$, on introduit la distribution suivante:

\Def \ Distribution de Fisher
\sskip 
Soient $Y_1\sim\chi_2(n_1)$ et $Y_2\sim\chi_2(n_2)$ deux VAs indépendantes, la loi de la VA $Z=\frac{Y_1/n_1}{Y_2/n_2}$ est appelée la distribution de Fisher et est notée $F(n_1,n_2)$.
\bskip 
Selon $H_0$, on a: $F_{n_1,n_2}=\frac{S^2_{1,n_1}}{S^2_{2,n_2}}\sim F(n_1-1,n_2-1)$ et donc la région de rejet:
$$W_{n_1,n_2}=\{F_{n_1,n_2}\notin [f_{n_1-1,n_2-1,\alpha/2},f_{n_1-1,n_2-1,1-\alpha/2}]\}$$
est de niveau de précision $\alpha$.
\bigskip 
\subsection{Analyse de la Variance}
La technique d'analyse de la variance (ANOVA) permet de tester l'homogénéité de plusieurs échantillons et donc de généraliser le test de Student de la section précédente. On suppose que les échantillons on la même variance inconnue. L'objectif de l'ANOVA est de construire un test pour les hypothèses suivantes:
$$H_0 = \{\mu_1 =... = \mu_k\} \ \ \text{et } H_1=\{\exists l,m : \mu_l\neq \mu_m\}.$$
On introduit par aillerus les notations suivantes: $n=\sum_{i=1}^kn_i$ et $\mathbb{X}_n=(\mathbb{X}_{1,n_1},...,\mathbb{X}_{k,n_k})$.
\bskip 
L'idée intuitive pour construire un test consiste à estimer $\ind \mu k$ à l'aide des moyennes empiriques. On rejete ainsi $H_0$ si ces quantités sont éloignées les unes des autres, on peut mesurer cette distance à l'aide de la moyenne empirique globale:
$$\bar X_{\cdot,\cdot} = \frac{1}{n}\sum_{i=1}^kn_i\bar X_{i,\cdot}$$
puis de considérer la statistique:
$$SSM=\sum_{i=1}^kn_i (\bar X_{i,\cdot} - \bar X_{\cdot, \cdot})^2 \ \ \text{telle que } \frac{SSM}{\sigma^2}\sim\chi_2(k-1)$$
qui donne la région de rejet $W_n=\{SSM\ge a_n\}$. 
\sskip 
Pour déterminer $a_n$, on introduit l'estimateur de la variance $\hat \sigma^2_{i,n_i}=\frac{1}{n_l}\sum_{i=1}^{n_l}(X_{l,i}-\bar X_{l,\cdot})^2$ afin de définir:
$$SSE = \sum_{i=1}^k n_i\hat \sigma^2_{i,n_i} \ \  \text{telle que } \frac{SSE}{\sigma^2}\sim\chi_2(n-k).$$
On obtient alors, selon $H_0$, $F_n = \frac{SSM/(k-1)}{SSE/(n-k)}$ est une statistique suivant une loi de Fisher de $k-1$ et $n-k$ degrés de liberté.
\sskip 
Finalement, on peut rejeter $H_0$ dès que $F_n \ge f_{k-1,n-k,1-\alpha}$ avec une précision $\alpha$.
\end{document}