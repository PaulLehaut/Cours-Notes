\documentclass{article}

\title{Contrôle des systèmes dynamiques}
\author{Paul Lehaut}
\usepackage{mesraccourcis}

\begin{document}
\newcommand{\AC}{AC([0,T],\R^n)}

\maketitle
\newpage
\tableofcontents
\newpage
On s'intéresse à $\frac{dx}{dt}=f(t,x(t),u(t))$ où $x(t)$ désigne l'état du système et $u(t)$ le contrôle.
On s'intéressera tout d'abord à: $x: [0,T]\longrightarrow \R^n, \text{ et } u\in L^{\infty}([0,T],\mathcal{U}) \ \text{où }\mathcal{U} \text{ désigne un sous-ensemble d'un \ev.}$
\bskip 
Les trois questions pricipales sont:
\bigskip

-Question de contrôlabilité: pour $x_0,x_1\in\R^n$, T fini, existe-t-il une fonction de contrôle $u$ telle que: $x_u(T)=x_1$?
\bigskip

-Question de contrôle optimal: on suppose qu'il existe au moins un contrôle emmenant le système de $x_0$ à $x_1$ en un temps T, existe-t-il alors un contrôle qui permet de réaliser ce passage en minimisant un critère ?
\bigskip

-Question de stabilisation: un état $x*$ étant donné, peut-on construire un contrôle $u(t)$ de type $k(x(t))$ (contrôle en boucle fermée) qui permet de ramener le système de l'état $x_0$ à l'état $x*$ en un temps T ?
\bigskip

\section{Contrôlabilité des systèmes dynamiques}
\subsection{Cas des systèmes linéaires autonomes}
Un système linéaire autonome est un système de la forme:
$$(I)\ \frac{dx}{dt}(t)=Ax(t)+Bu(t), \ \text{avec} \ x(0)=x_0, \ A\in\M_n(\R), \ \text{et } B\in\M_{n,k}(\R).$$
La solution de ce système est explicite donnée par:
$$x(t)=e^{tA}x_0+\int_0^te^{(t-s)A}Bu(s)ds.$$
Le problème de contrôle en un temps T peut alors se réécrire, $\forall x_0,x_1\in \R^n$, trouver $u\in L^{+\infty}([0,T],\R^k)$ telle que:
$$\int_0^Te^{(t-s)A}Bu(s)ds=\Phi_T(u)=x_1-e^{TA}x_0$$
soit encore montrer que l'application linéaire:
$$\Phi_T:\begin{aligned}
    L^{+\infty}([0,T],\R^k) &\longrightarrow \ \ \ \ \R^n&\\
    u\ \ \ \ &\longmapsto \int_0^Te^{(t-s)A}Bu(s)ds
\end{aligned}$$
est surjective.
\bigskip

\Theo \ Critère de Kalman
\sskip 
Soit $A\in \M_n(\R)$, soit $B\in\M_{n,k}(\R)$, pour $T>0$, le système $(I)$ est contrôlable en un temps T si et seulement si la matrice $C=(B, AB, ..., A^{n-1}B)$ est de rang maximal n.
\bigskip 

\Dem
\smallskip

Montrons tout d'abord que $\Phi_T$ n'est pas surjective si la matrice C n'est pas de rang maximal.
\sskip
En effet, si C n'est pas de rang maximal, alors il existe $y\in\R^n$ non nul tel que $y^TC=0$ donc $y^TA^jB=0$ pour tout $j\in[|0,n-1|]$. 
\newline
Or, le théorème de Cailey-Hamilton donne: $A^n=\sum_{k=0}^{n-1}a_kA^k$ donc $y^TA^nB=0$ et donc, par récurrence: $y^TA^jB=0$ pour tout $j$.
\newline
On calcule alors: 
$$y^T\Phi_T(u)=y^T\int_0^Te^{(T-s)A}Bu(s)ds=\int_0^Ty^T\sum_{k=0}^{\pinf} \frac{(T-s)^k}{k!}A^kBu(s)ds=0$$
donc, pout tout contrôle $u$, $\Phi_T(u)$ appartient à l'espace orthogonal à $y^T$ qui est un vecteur non nul, donc $\Phi_T$ n'est pas surjective.
\smallskip

Montrons désoramais que, si $\Phi_T$ n'est pas surjective, alors la matrice $C$ n'est pas de rang maximal.
\sskip
Comme $\Phi_T$ n'est pas surjective, alors il existe $y$ non nul appartenant à l'espace orthogonal à $Im \Phi_T$. On s'intéresse alors à la fonction $u$ définie par:
$$u(s)=B^Te^(T-s)A^Ty$$
qui est clairement une fonction de $L^\infty([0,T],\R^k)$, il vient alors:
$$y^T\Phi_T(u)=\int_0^Ty^Te^{A(T-s)}BB^Te^{(T-s)A^T}yds=\int_0^T<y^Te^{(T-s)A}B,y^Te^{(T-s)A}B>ds=0$$
on constate donc que, pour tout $s\in[0,T]$, on a: $y^Te^{(T-s)A}B=0$, donc, en particulier, en notant $f:s\mapsto y^Te^{(T-s)A}B$, alors:
$$f(T)=y^TB=0, \ f'(T)=-y^TAB=0,...\ \text{et} \  f^{(n-1)}(T)=(-1)^{n-1}y^TA^{n-1}B=0$$
donc $y^T$ appartient au noyau de $C$ qui n'est donc pas de rang maximal.
\smallskip

D'où le résultat.
\bskip
On constate par ailleurs que ce critère ne dépends pas du temps T.
\bigskip

\Theo 
\sskip
Le système $(I)$ est contrôlable si et seulement si la matrice
$$G_T=\int_0^Te^{(T-s)A}BB^Te^{(T-s)A^T}ds$$
est inversible, un contrôle admissible est alors:
$$u(t)=B^Te^{(t-s)A^T}G_T^{-1}x(x_1-e^{TA}x_0).$$
\bskip 
L'ensemble des contrôles admissibles réalisant le passage de $x_0$ à $x_1$ en temps T est:
$$\{u\in L^{+\infty}([0,T], \R^k) \ | \ \Phi_T(u)=x_1-e^{TA}x_0\}.$$
\bigskip 

\subsection{Contrôle des systèmes non linéaires}
On s'intéresse aux systèmes de la forme:
$$(II)\ \frac{dx}{dt}(t)=f(t,x(t),u(t)), \ \text{avec} \ x(0)=x_0.$$
On peut tout d'abord se demander si, pour $u\in L^{+\infty}([0,T], \R^k)$ fixé, le problème de Cauchy $(II)$ a-t-il une solution et une seule sur l'intervalle $[0,T]$ ?
Autrement dit le problème de Cauchy qui consiste à chercher $x:[0,T]\longrightarrow  \R^n$ telle que:
$$\frac{dx}{dt}(t)=F(t,x(t)), \ \text{avec} \ x(0)=x_0 \ \text{et} \ F(t,x(t))=f(t,x(t),u(t))$$
a-t-il une unique solution ?
\bskip 
Dans le cas standard, si $F$ est continue et lipschitzienne en $x$ alors l'équation admet une unique solution.
Néanmoins en théorie du contrôle on peut tout à fait considérer des contrôles discontinus qui implique en général la discontinuité de $F$.
\bigskip

\Def 
\sskip
On note $\AC:=\{x\in \mathcal{C}^0([0,T],\R^n) \ | \ \frac{dx}{dt}\in L^1([0,T],\R^n) \}$, la dérivée est ici à prendre au sens des distributions.
\bskip 
Pour tout $x\in\AC$, on a: $x(t)=\int_0^t \frac{dx}{dt}(s)ds+x_0$.
\bigskip

\Theo 
\sskip
Soit $F:[0,T]\times \R^n \longrightarrow \R^n$ mesurable et vérifiant:
$$\exists c_T\in L^1([0,T], \R_+): \ ||F(t,x)-F(t,y)||\le c_T(t)||x-y|| \ \ \text{(Pseudo lipschitzianité)}$$
et:
$$\forall x\in \R^n, \ \exists\beta_x\in L^1([0,T], \R_+): \ |F(t,x)|\le\beta_x(t)$$
alors le problème $(II)$ admet une unique solution $x\in\AC$ et $x(t)=x_0+\int_0^tF(s,x(s))ds$.
\sskip
En remplaçant l'hypothèse de pseudo lipschitzianité dans le théorème précédent par une pseudo lipschitzianité locale:
$$\forall x_0\in \R^n,\ \exists r>0 \ \text{et} \ \exists C_{T,x_0}\in L^1([0,T],\R^n): \ \forall (x,y)\in B(x_0,r)^2, \ ||F(t,x)-F(t,y)||\le C_{t,x_0}(t)||x-y||$$
alors il existe $0<T_0\le T$ tel qu'il existe une unique solution $x\in \mathcal{C}^0([0,T_0[,\R^n)$ et on a:

-ou bien $T_0=T$ et $x\in \AC$

-ou bien $|x(t)|\to_{t\to T_0}+\infty$.
\bigskip 

\Def 
\sskip 
On dit que le système $\dot{x} = f(t,x(t),u(t))$ (A) est contrôlable en temps T si pour tout $x_0,x_1\in\R^n$, il existe $u\in\mc C^{\infty}([0,T],\R^k)$ tel que:

-(A) admet une unique solution dans $\AC$

-$x(T)=x_1$.
\bskip 
Il est souvent pertinent de considérer une notion locale de la contrôlabilité.
\bigskip 

\Def \ Contrôlabilité autour d'une trajectoire
\sskip 
Soit $\bar u\in L^{\infty}([0,T],\R^k)$ et $\bar x \in \AC$ la trajectoire associée à $\bar u$. (A) est contrôlable autour de $(\bar u, \bar x)$ si ,pour tout $\epsilon>0$, il existe $\nu$ strictement positif tel que:
$$\forall x_0\in B(\bar x_0, \nu), \ \forall x_1\in B(\bar x_1, \nu), \ \exists u \in L^{\pinf}([0,T], \R^k): \ ||u-\bar u||\le \epsilon$$
et la trajectoire $x$ engendrée par $u$ à partir de $x_0$ vérifie $x(T)=x_1$.
\bskip 
Si $||u-\bar u||<<1$ et sous les bonnes conditions sur $f$, alors: $||x-\bar x|| <<1.$
\bskip 
En posant $u:=\bar u +v$ et $x:=\bar x+y$, la dynamique $\dot x=f(t,x(t),u(t))$ peut être approché par la dynamique linéarisé:
$$\dot y\approx \frac{\partial f}{\partial x}(t,\bar x(t), \bar u(t))y(t)+\frac{\partial f}{\partial u}(t,\bar x(t), \bar u(t))v(t) \ \ (B).$$
\bigskip 

\Def 
\sskip 
Le système (B) est appelé le linéarisé de (A) autour de la trajectoire $(\bar x, \bar u)$.
\bskip 
Si ($\bar x, \bar u)$ est stationnaire (point d'équilibre de la dynamique) alors le système linéarisé est autonome.
\bigskip 

\Theo \ Test linéaire
\sskip 
Si le système linéarisé autour d'un point stationnaire $(\bar x, \bar u)$ est contrôlable, alors le système non-linéaire de départ est localement contrôlable autour de ce point d'équilibre.
\bskip 
Il s'agit d'une condition suffisante mais non nécessaire.
\newpage
\section{Stabilité et Stabilisation des Systèmes Dynamiques}
\subsection{Trois Notions de Stabilité}
On considère un système dynamique autonome $\dot x = F(x(t))$ avec $F:\R^n\longrightarrow\R^n$ continue et localement lipschitzienne (ces hypothèses sont conservées sur $F$ dans toute cette section sauf indications contraires).
\bigskip 

\Def 
\sskip 
Un point $x_*$ est appelé un point d'équilibre de la dynamique si $F(x_*)=0$, il est dit:
\begin{itemize}
    \item Stable si: $\forall \epsilon>0, \exists \delta>0: ||x_0-x_*||<\delta\implies||x(t)-x_*||\le\epsilon$.
    \item Asymptotiquement stable si: $x_*$ est stable et si $\forall x_0 \in \R^n, ||x(t)-x_*||\longrightarrow_{t\to\pinf} 0$.
    \item Exponentiellement stable si: $x_*$ est stable et s'il existe $\delta>0$ et $c\ge0$ tels que: $||x(t)-x_*||\le Ce^{-\delta t}||x_0-x_*||.$
\end{itemize}
\bigskip 
On a évidemment exponentiellement stable qui implique asymptotiquement stable qui implique ensuite stable.
\bigskip 

\subsection{Stabilité des Systèmes Linéaires}
On considère le cas particulier d'un système linéaire $\dot x = Ax$ avec $A\in\M_n(\R)$ inversible. Le seul point d'équilibre de la dynamique est donc le point $0$. Par ailleurs ce système a pour solution:
$$x(t)=e^{tA}x_0$$
l'étude de la stabilité se ramène donc à l'étude de $(e^{tA}, \ t\ge0)$.
\bigskip 
\subsubsection{Cas des matrices diagonalisables}
On suppose que $A$ est diagonalisable, on peut alors écrire:
$$e^{tA}=P\begin{pmatrix}
    e^{t\lambda_1} & & 0\\
     & \ddots &\\
    0& & e^{t\lambda_n}
\end{pmatrix}P^{-1}$$
le système est donc stable si $Re(\lambda_j)\le 0$, pour tout $j$. Il est exponentiellement stable si et seulement si $Re(\lambda_j)<0$, pour tout $j$.
\bigskip 
\subsubsection{Cas des matrices non diagonalisables} 
On appelle bloc de Jordan une matrice du type:
$$J_j(\lambda)=\begin{pmatrix}
\lambda & 1 & & 0 \\
& \ddots &\ddots & \\
&&\ddots&1\\
0& & &\lambda 
\end{pmatrix} = \lambda I_j + N_j \ \ \text{avec} \ \ N_j=\begin{pmatrix}
    0 & 1 & & 0\\
    & & \ddots &\\
    &&&1\\
    0&&&0
\end{pmatrix}$$
on remarque donc que: 
$$e^{tJ_j(\lambda)}=e^{t\lambda}\begin{pmatrix}
    1 & t & \hdots & \frac{t^{j-1}}{(j-1)!}\\
    & \ddots & \ddots & \\
    &&\ddots & t\\
    0&&&1
\end{pmatrix}$$
\bigskip 

\Theo \ Décomposition de Jordan 
\sskip 
Toute matrice carré $A$ peut s'écrire sous la forme:
$$A=P\begin{pmatrix}
    J_{j_1}(\lambda_1) & & 0\\
    & \ddots & \\
    0&& J_{j_r}(\lambda_r)
\end{pmatrix}P^{-1}.$$
\bigskip 
Ce théorème permet d'écrire l'égalité suivante:
$$e^{tA}=P\begin{pmatrix}
    e^{tJ_1(\lambda_1)} & &0\\
    &\ddots&\\
    0&&e^{tJ_r(\lambda_r)}
\end{pmatrix}P^{-1}.$$

\Prop 
\sskip 
Soient $\ind\lambda r$  les valeurs propres de $A$ comptées avec leur multiplicité algébrique.
\sskip 
Si $Re(\lambda_j)<0$ pour tout $j$, le système est exponentiellement stable. Si $Re(\lambda_1)=0$ et $Re(\lambda_j)<0$ pour tout $j\in[|2,r|]$ le système est simplement stable.
\bigskip 

\Dem
\sskip 
Si $Re(\lambda_j)<0$ pour tout $j$, alors, comme:
$$||e^{tJ_j(\lambda)}||_1=e^{tRe(\lambda)}(\sum_{k=0}^{j-1}\frac{t^k}{k!})$$
converge exponentiellement vite vers 0 lorsque $t$ tend vers $\pinf$, il en est donc de même pour $||e^{tA}||_1$.
\newline
Si $Re(\lambda_1)=0$ et $Re(\lambda_j)<0$ pour tout $j\in[|2,r|]$ alors:
$$e^{tA}=P\begin{pmatrix}
    e^{\lambda_1t} & & &0\\
    & e^{tJ_2(\lambda_2)}&&\\
    &&\ddots&\\
    0&&&e^{tJ_r(\lambda_r)}
\end{pmatrix}P^{-1}$$
donc le sytème 'tourne' asymptotiquement et est donc simplement stable.
\bigskip 

\subsection{Stabilité des Systèmes Non Linéaires}
Soit $x*$ un état d'équilibre de $\dot x = F(x)$.
\bigskip 

\Def 
\sskip 
$x*$ est dit:
\begin{itemize}
    \item localement asymptotiquement stable si: il est stable et s'il existe $\delta>0$ telle que $$||x_0-x*||\le\delta\implies ||x(t)-x*||\longrightarrow_{t\to\pinf}0.$$
    \item localement exponentiellement stable si: il est stable et s'il existe $\delta,\mu>0$ telles qu'il existe $C\ge 0$ telle que: $$||x_0-x*||\le\delta\implies||x(t)-x*||\le Ce^{-\gamma t}||x_0-x*||.$$
\end{itemize}
Comme précédement un point localement exponentiellement stable est localement asymptotiquement stable.
\bigskip 

\Theo \ Test linéaire 
\sskip 
Soit $x*$ un point d'équilibre du système non-linéaire $\dot x =F(x)$ avec $F$ de classe $\mc C^1$ et $\dot y=Ay$ le système linéarisé autour de $x*$ (avec $A=\frac{\partial F}{\partial x}(x*)$ la jacobienne de $F$ en $x*$).
\newline 
Si le système $\dot y=Ay$ est asymptotiquement/exponentiellement stable, alors le système non-linéaire est localement asymptotiquement/exponentiellement stable autour de $x*$.
\newpage
\section{Fonctions de Lyapunov}
On s'intéresse au problème $\dot x = F(x(t))$ (I) avec $F\in \mc C^1(\R^n,\R^n)$ et $x^*$ tel que $F(x^*)=0$.
\sskip 
On cherche à savoir si $x^*$ est stable (voire asymptotiquement ou exponentiellement stable).
\bigskip 

\Def \ Fonction de Lyapunov
\sskip
On appelle V une fonction de Lyapunov pour le système (I) (et l'équilibre $x^*$) si:
\begin{itemize}
    \item $V\in \mc C^1(\R^n,\R_+)$
    \item $\forall x\in\R^n\backslash\{x^*\}, \ V(x)>V(x^*)$
    \item $\lim_{||x||\to\pinf}V(x)=\pinf$
    \item $\forall x\in \R^n, \ \nabla V(x)\cdot F(x)\le 0$
\end{itemize}
\bigskip
Soit $x\in\mc C^1([0,T])$ une solution de (I), alors:
$$V(x(t))\le V(x(0)) \ \ \text{et} \ \ \frac{dV}{dt}(x(t))=\nabla V(x(t))\cdot \dot x(t)=\nabla V(x(t))\cdot F(x(t))\le 0$$
\bigskip 

\Theo \ Lyapunov
\sskip 
S'il existe une fonction de Lyapunov pour le système (I) alors $x^*$ est un équilibre stable.
\bigskip 

\Def \ Fonction de Lyapunov stricte
\sskip
Si V est une fonction de Lyapunov pour (I) et que:
$$\forall x\in \R^n \backslash \{x^*\}, \ \nabla V(x)\cdot F(x)<0$$
alors V est dite stricte.
\bigskip 

\Theo 
\sskip 
S'il existe une fonction de Lyapunov stricte pour le système (I) alors $x^*$ est un équilibre asymptotiquement stable.
\bigskip 

\Def 
\sskip 
On dit que V est une fonction de Lyapunov exponentielle si:
\begin{itemize}
    \item $V\in \mc C^1(\R^n,\R_+)$
    \item $\exists \alpha,C,c >0: \ \forall x\in \R^n, \ c||x-x*||\le V(x)\le C||x-x^*||^\alpha$
    \item $\exists \alpha >0: \ \forall x\in \R^n, \ \nabla V(x)\cdot F(x)\le-\alpha V(x)$
\end{itemize}
\bigskip 

\Theo 
\sskip 
S'il existe une fonction de Lyapunov exponentielle pour (I) alors $x^*$ est exponentiellement stable.
\bigskip 

\Theo \ La Salle 
\sskip 
Supposons qu'il existe une fonction de Lyapunov V pour (I), si on a $x$ une solution de (I) telle que: $\nabla V(x(t))\cdot F(x(t))=0$ pour tout $t\ge0$, alors: $x(t)=x^*$ pour tout $t\ge0$ et le système est asymptotiquement stable.
\newpage 
\section{Stabilisation des Systèmes Dynamiques}
On s'intéresse au système $\dot x =f(x(t),u(t))$ (I). On cherche à déterminer l'existence d'une fonction $g$ telle que, avec $u(t)=g(x(t))$ (contrôle feedback), le système $\dot x = f(x(t),g(t))$ (II) soit stable (voire asymptotiquement ou exponentiellement stable).
\sskip 
On note $F:x\mapsto f(x,g(x))$.
\bigskip 
\subsection{Systèmes Linéaires}
On considère $\dot x = Ax + BKx$ (III) (avec donc $K\in \M_{k,n}(\R)$), on cherche à établir l'existence de $K$ telle que $A+BK$ a des valeurs propres de partie réelle strictement négative uniquement.
\bigskip 

\Theo \ Placement des pôles
\sskip 
Si $(A,B)$ est contrôlable pour tout $P$ un polynôme de degré n, il existe $K$ tel que $A+BK$ a pour valeurs propres les racines de P.
\bigskip 

\underline{Corollaire:}
\sskip 
Si $(A,B)$ est contrôlable, alors, pour tout $\gamma>0$, il existe $K_\gamma$ telle que le système (III) est exponentiellement stable avec un taux de décroissance $\gamma$.
\bigskip 

\section{Systèmes Non-Linéaires}
On cherche simplement à savoir s'il existe $g$ tel qu'il existe une fonction de Lyapunov pour le système (II).
\newpage
\section{Contrôle Opitmal des systèmes dynamiques}
On s'intéresse à $\dot x =f(t,x(t),u(t))$ avec $x_0=x(0)$, on note $J(u)=\int_0^Tg(t,x(t),u(t))dt  + h(x(T))$, on cherche à trouver un contrôle optimal c'est à dire qui minimise $J$.
\bigskip 
\subsection{Principe de Minimum de Pontryagin}
Pour tout $t\in [0,T], \ u(t)=\arg\min_{v\in \mc U} H(t,x(t),p_u(t),v(\in \R^k))$.
\end{document}
