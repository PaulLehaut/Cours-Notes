\documentclass{article}
\usepackage{mesraccourcis}

\title{How Does It Work?}
\author{Paul Lehaut}


\begin{document}

\maketitle
\newpage
\tableofcontents
\newpage
\section{Introduction}
Schematically : DL $\subset$ ML $\subset$ AI.
\begin{itemize}
    \item AI : Set of theories/algorithms allowing computers task that may have required human intelligence.
    \item ML : Focuses on algorithms that improve automatically through experience. It studies function approximation where parameters are estimated from data rather than hard-coded.
    \item DL : A subset of ML where the approximating functions are artificial neural networks with multiple layers, allowing for hierarchical representation learning.
\end{itemize}
\bigskip
\subsection{Machine Learning}
Let's precise, first ML :
\begin{itemize}
    \item The goal is to approximate a relation $\Prob $ between inputs $\mc X$ and outputs $\mc Y$ by using a function of our data : $h\in \mc H:\mc X\longrightarrow\mc Y$, where $\mc H$ is the set of available functions (the hypothesis).
    \item $h$ will need to minimize what is called the $\mathbf{True \ Risk}$ : $$ R(h) = \E_{(x,y)\sim \Prob}(l(h(x),y)) = \int_{\mc X\times \mc Y}l(h(x),y)d\Prob(x,y)$$ where $l$ is a convex loss function.
    \item The main problem is to compute $R(h)$ while $\Prob$ is unknown.
\end{itemize}
To tackle that problem, we use a Monte Carlo estimate based on our data $S = ((x_i,y_i))_{1\le i \le n}\sim \Prob$ iid, we get the $\mathbf{Empirical \ Risk}$:
$$\hat R_S (h) = \frac{1}{n}\sum_{i=1}^nl(h(x_i),y_i)$$
we then choose $\hat h_S$ such that : $\hat h_S\in \text{argmin}_{h}\hat R_S(h)$.
\sskip 
To avoid overfitting $(\hat R_S(\hat h_S)\approx 0 \ \text{while} \ R(\hat h_S)>>0)$ we admit that Generalization Gap $\approx \sqrt{\frac{|\mc H|}{n}}$.
\bskip 
We can then decompose the error of our learned predictor $\hat h_S$ into two orthogonal components relative to the Bayes optimal predictor $h^*$ (the theoretical limit of performance):
$$h^*(x)= \text{argmin}_y \E_{Y\sim \Prob(\cdot|\ X=x)}(l(y,Y)).$$
The $\mathbf{Excess \ Risk}$ is:
$$R(\hat h_S) - R(h^*)= (R(\hat h_S) - \inf_{h\in\mc H}R(h)) + (\inf_{h\in\mc H}R(h) - R(h^*))$$
\begin{itemize}
    \item $\mathbf{Bias}$ : $\inf_{h\in\mc H}R(h) - R(h^*)$ is the capacity of our hypothesis to approximate the truth.
    \item $\mathbf{Variance}$ : $R(\hat h_S) - \inf_{h\in\mc H}R(h)$ how far is our chosen $\hat h_S$ from the best $h\in \mc H$ (due to finite sampling).
\end{itemize}
This brings the following problem : as we increase the complexity of our model $\mc H$, we reduce the bias but we increase the variance.
\sskip 
To deal with this trade-off, we may try to minimize $\hat R_S(h) + \lambda \Omega(h)$ instead of $\hat R_S(h)$, where $\Omega(h)=||h||_{L^2}$ for instance.
\bskip 
Typically $\mc H$ includes, for ML : 
\begin{itemize}
    \item linear/affine models : $h(x)=W\phi(x) + b$ where $b\in \R^d, \ W\in \M_{d,k}(\R)$ and $\phi:\mc X\longrightarrow R^k$, used to deal with logistic regression for classification
    \item feature maps + linear head : $h(x)=w^T\phi(x)$ where $w\in \R^d$ and $\phi:\mc X\longrightarrow R^d$ 
    \item kernel methods : $h_\alpha(x) = \sum_{i=1}^n \alpha_i k(x,x_i)$ for a kernel $k$
\end{itemize} 
\subsection{Deep Learning}
The main difference lies in the choice of $\mc H$, we will here use $\mathbf{Deep \ Neural \ Networks}$. A neural network is a directed acyclic graph which describes a composition of differentiable functions.
\sskip 
For a network with $L$ layers we have, for instance: $f_\theta(x) = f_L(f_{L-1}(...(f_1(x)...)))$ where each layer performs, in general, an affine transformation followed by a non-linear activation function (in most cases ReLU: $\sigma(z)=\max(z,0)$):
$$f_l(x) = \sigma (W_l f_{l-1}(x) + b_l).$$
$\theta = \{W_l,\ b_l\}_{l=1}^L$ are the learnable parameters.
\sskip 
Early layers learn low-level features and deeper layers capture higher-level, more abstract features.
\bigskip 
\bigskip 
\subsection{Representation Learning vs Feature Engineering}
This disctinction is the fundamental reason why DL overtook classical ML in perceptual tasks.
\sskip 
Indeed, let's say we seek a predictor $f(x)=w\cdot \phi(x) \approx y$ with a feature map $\phi : \mc X\longrightarrow \R^k$. $\phi$ transforms raw data into a more useful representation, $w$ is usually a linear classifier.
\sskip 
In classical ML (Feature Engineering), we need to fix $\phi$, the problem is then to find the optimal weights $w$ given $phi$. Thus, the performance is bounded by the quality of $\phi$.
\sskip 
In DL (Representation Learning), we treat $\phi$ as a parameterized function $\phi_\theta$ (a NN) and we learn both $\theta$ and $w$ at the same time. The model discovers the optimal features by itself and for the specific task we are dealing with. On the other hand, it requires more data to perform.
\bigskip 
\section{Feedforward Networks}
Often called Multi-Layer Perceptrons (MLPs), they are the foundational architecture of DL.
\sskip 
The $\mathbf{Universal \ approximation \ theorem}$ implies that a feedforward network with a single hidden layer can approximate any continuous function on a compact subset of $\R^n$, given appropriate activation functions.
\bskip 
Let's study an MLPs with $L$ layers, the network defines a function $f:\R^{d_{in}}\longrightarrow\R^{d_{out}}$ so that:
$$x_0 = x, \ x_L = f(x) \ \ \text{and, for each layer l:\ } x_l = \sigma_l(W_l x_{l-1}+ b_l) = f_l(x_{l-1},\theta_l).$$
The non-linearity introduced by $\sigma$ allow the network to warp the input space to underline non-linear relationships between the features. Common choices for $\sigma$ are ReLU or sigmoid ($\sigma(z)=\frac{1}{1+e^{-z}}$).
\sskip 
We define $\theta$ as the set of trainable parameters.
\sskip
Training an MLP leads to minimizing the non-convex function:
$$J(\theta) = \frac{1}{n}\sum_{i=1}^nl(f_\theta(x_i),y_i) \ (J_{MSE}(\theta)= \frac{1}{n}\sum_{i=1}^n||f_\theta(x_i)-y_i||^2 \ \ \text{for instance.})$$
To find the optimal $\theta$ that minimizes $J(\theta) = \hat R_S(f_\theta)$ we use, in most cases, the $\mathbf{Stochastic \ Gradient \ Descent}$ (SGD):
$$\theta_{t+1}=\theta_t - \eta_t\nabla_\theta l(f_\theta(x_{i(t)},y_{i(t)}))$$
where $i(t)$ is a random index.
\sskip
Computing the gradient is done via $\mathbf{Backpropagation}$ which is an application of the chain rule :
$$\frac{\partial J}{\partial \theta_k}(\theta_k) = \frac{\partial J}{\partial x_k}(x_k)\frac{\partial x_k}{\partial \theta_k}(x_{k-1},\theta_k)=\frac{\partial J}{\partial x_k}(x_k)\frac{\partial f_k}{\partial \theta_k}(x_{k-1},\theta_k).$$
Where $ \frac{\partial J}{\partial x_k} (x_k)$ is computed in the backward pass and $x_k$ in the forward pass.
\bskip
Networks are often regularized by the squared L2 loss of the learnable parameters, leading to minimizing for instance : 
$$J(\theta) = J_{MSE}(\theta) + \lambda ||\theta||_{L^2} \ \ \text{where } \lambda \ \text{is a scalar hyper-parameter.}$$
Most of the time we don't compute the full gradient but a $\mathbf{Batch}$ stochastic gradient descent : 
$$\theta_{t+1}=\theta_t - \eta_t\frac{1}{B}\sum_{n=1}^B\nabla_\theta l(f_\theta(x_{i(t,b)},y_{i(t,b)}))$$
where B is the batch size. We use this method to accelerate gradient computing (by computing in parallel on GPUs). Moreover, this method converges faster.
\sskip 
$\eta_t$ is frequently called the $\mathbf{Learning \ Rate}$, the most common strategy is to pick a high learning rate (that still leads to steady loss decrease, typically $10^{-2},\ 10^{-3}$) and keep it constant until convergence. The idea is that a hight lr increases chances of getting out of local minima. Once the losses seems to have converged, we can decrease the lr.
\bskip
To improve the training stability and convergence, we often use $\mathbf{Batch \ Normalization}$ (BN), which both has estimated and learned parameters. It is frequently implemented just before non-linearities and has different types of behavior during training and inference.
\begin{itemize}
    \item Training : the estimated parameters are, for a batch $B$, his mean and standard deviation $\mu_B=\bar x_n$ and $\sigma_B^2 = \frac{1}{n}\sum_{i=1}^{n}(x_i - \mu_B)^2$, we note $a$ and $b$ its learnable parameters and its output is : 
    $$ y_i = a\frac{x_i - \mu_B}{\sigma_B} + b$$
    \item Testing : at this point, data might not be available in batches, we thus keep running estimates of $\mu$ and $\sigma$, which we may update during iteration after sampling a batch $B$ by : 
    $$\mu\leftarrow m\mu + (1-m)\mu_B, \ \ \sigma\leftarrow m\sigma + (1-m)\sigma_B \ \ \text{where } m \ \text{is a hyperparameter close to } 1 \ (0.9 \ \text{for instance}).$$
\end{itemize}
\subsection{Convolutional Neural Networks (CNNs)}
We consider a tensor $X\in\R^{C_{in}\times H\times W}$ as our input (Channels, Height, Width). Our trainable parameters form a kernel $K\in \R^{C_{out}\times C_{in} \times k\times k}$, a single output channel is given by:
$$Y_{u,v} = (K\star X)_{u,v} = \sum_{c=1}^{C_{in}}\sum_{i=1}^k\sum_{j=1}^k K_{c,i,j}X_{c,u+i-1,v+j-1} + b.$$
A standard CNN is thus a sequence of convolution, non-linearity and pooling repeated L times.
\section{Transformer Architecture}
We first introduce a new type of architecture.
\subsection{Recurrent Neural Networks (RNNs)}
Unlike MLPs where the internal state $h_l$ depends only on the current input $h_{l-1}$, an RNN $\mathbf{maintains \ a \ 'memory'}$.
\sskip 
Let $x = (\ind x T) $ be a sequence of inputs where $x_t\in \R^d$. Then the RNN maintains a hidden state $h_t\in \R^k$ according to:
$$h_{t+1} = \sigma(W_{hh}h_t + W_{xh}x_{t+1} + b_h) \ \ \text{and} \ \ y^*_t = \phi(W_{hy}h_t + by) \ \ \text{the output at time } t$$
To train this network we may use a total loss function: $\mc L = \sum_{t=1}^T l(y^*_t,y_t)$.
To update $W_{hh}$, we need to compute $\frac{\partial \mc L}{\partial W_{hh}}$, which means computing, for each time step $t$:
$$\frac{\partial l_t}{\partial W_{hh}} = \sum_{i=1}^t\frac{\partial l_t}{\partial y^*_t}\frac{\partial y^*_t}{\partial h_t}\frac{\partial h_t}{\partial h_i}\frac{\partial h_i}{\partial W_{hh}}$$
or: 
$$\frac{\partial h_t}{\partial h_i}=\prod_{j=i+1}^t\frac{\partial h_j}{\partial h_{j-1}}\approx \prod_{j=i+1}^t W_{hh}$$
which leads to a matrix exponentiation. 
\sskip 
There are two cases:
\begin{itemize}
    \item Eigenvalues $\lambda$ of $W_{hh}$ are such that $\max_\lambda|\lambda|<1$, then $\prod_{j=i+1}^t W_{hh}$ tends to zero and the network forgets long-term dependencies
    \item $\max_\lambda|\lambda|>1$ and the gradient explodes.
\end{itemize}
This spectral instability is the main reason why RNNs are difficult to train.
\sskip 
To tackle this problem we introduce the Transformer architecture.
\bigskip 
\subsection{Transformer Architecture}
Let $X\in \M_{n,d}(\R)$ be our input matrix of $n$ vectors embedded in $d$ dimensions. It is important to notice that transformer architecture treats the input as a set and does not have an inherent notion of order.
\sskip 
However, it is possible to fix it by injecting $\mathbf{Positional \ Encodings}$ into X (typically vectors containing $\sin\backslash\cos$ frequencies).
\bskip 
First, we project X into three distinct subspaces:
\begin{itemize}
    \item $\mathbf{Queries}$ $(Q=XW_Q)$ : what is $Q_i$ looking for ?
    \item $\mathbf{Keys}$ $(K=XW_K)$ : what does $W_i$ mean ?
    \item $\mathbf{Values}$ $(V=XW_V)$ : what does $V_i$ give if selected ?
\end{itemize}
We often use two different sets of inputs, one to compute queries, and the other one to compute both keys and values (it is called cross-attention).
\sskip
The $\mathbf{Attention \ Mechanism}$ works by mapping a query and set of key-value to an output:
$$\text{Attention}(Q,K,V)=\text{softmax}(\frac{QK^T}{\sqrt{d_k}})V \ \ \text{with} \ \ \text{softmax}(\frac{QK^T}{\sqrt{d_k}})_{i,j} = \frac{(\exp(QK^T)_{i,j}/\sqrt{d_k})}{\sum_{k=1}^n \exp((QK^T)_{i,k}/\sqrt{d_k})}$$
\begin{itemize}
    \item $QK^T$ computes the dot product between queries and keys, it measures the alignment between them
    \item $\frac{1}{\sqrt{d_k}}$ is the scaling factor if components of $Q$ and $K$ are independent with mean 0 and variance 1, their dot product has also a mean of 0, we then define $d_k$ as its variance so that the global variance is 1
    \item finally, the output $\text{Attention}(Q,K,V)_i$ is a convex combination of all value vectors weighted by their relevance.
\end{itemize}
Still, a single dot product only captures one type of similitude. To capture multiple, we run the process multiple times with different matrices : $\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1,...,\text{head}_h)W^O$.
\bigskip
\subsection{Language Models}
Most Language models are transfromers.
\sskip 
The problem is that text can't easily be converted into a continuous fixed-sized representation. We then represent it as a sequence of elementary objects named $\mathbf{Tokens}$. A successfull method is to use vocabularies of tokens in-between characters and words (referred as a sub-word tokenization). A popular sub-word tokenization is the $\mathbf{Byte \ Pair \ Encoding}$ which defines tokens by : 
\begin{itemize}
    \item Define each charachters as a token
    \item Iterate until reaching a target number of tokens : \begin{itemize}
        \item For each pair of successive tokens, count the number of occurences 
        \item The most common paire defines a new token 
        \item Replace every possible pair with the new token.
    \end{itemize}
\end{itemize}
There is typically $30-40$k tokens in modern deep networks, which are then projected into a space of typical dimension $512-4096$.
\bskip 
A LM obviously needs a positional encoding, a good way to add it is to use sine and cosine position embeddings $PE$. If we denote by $d$ the tokens dimension, then $PE(pos)\in \R^d$ is defined for position $pos$ of the token and $i\in\{1,...,\frac{d}{2}\}$ by : 
$$PE(pos)_{2i} = \sin(\frac{pos}{10000^{2i/d}}), \ PE(pos)_{2i+1} = \cos(\frac{pos}{10000^{2i/d}}).$$
We then process with two transformer architectures:
\begin{itemize}
    \item $\mathbf{Encoder}$ : it takes as input a set of N tokens and outputs N tokens. It applies $K$ transformers blocks, all with the same architecture, to the tokens:\begin{itemize}
        \item A linear layer predicting key, queray and value tokens and computing multi-head-attention over them 
        \item The output for each token is added to the input token 
        \item Layer normalization is applied to the results
        \item Outputs are then processed (independently) by a small MLPs (2 layers) with residual connection 
        \item Then a last layer normalization.
    \end{itemize}
    \item $\mathbf{Decoder}$ : It works as the encoder but in each block, between the first linear layer and the MLP is added a multi-head-cross-attention layer (query tokens are computed from the current decoder tokens, while the keys and values are computed from the output tokens of the encoder).
\end{itemize}
\bigskip
\subsection{Image classific Vision Transformers : ViT architecture}
The Vit architecture is a transformer encoder. Here, we deal with linear embedding of image patches with 2D positional encoding. Moreover, the network takes as an input an additional learnable token : the $\mathbf{class \ token}$. The only output considered to predict image classes is the output corresponding to the class token.
\end{document}